{
    "queries": {
        "bca54d06-4e9d-4b3d-a2bc-530ad4ae7b49": "How does Daron Acemoglu acknowledge the contributions of his co-authors in his essay on the harms of AI?",
        "9e8a98f7-d3ef-4665-9c79-b3b8f4e83ebf": "What sources of financial support does Daron Acemoglu gratefully acknowledge in his essay on the harms of AI?",
        "bb27adeb-6857-4db6-9bef-2ca25de50b2a": "How does Daron Acemoglu argue that the current trajectory of AI technologies, if left unregulated, may lead to various social, economic, and political harms?",
        "6c44ed90-aecd-478f-ab33-707b8312ef14": "According to Acemoglu, what are some of the potential negative consequences of AI technologies being used and developed in a way that empowers corporations and governments against workers and citizens?",
        "2c6710d4-df01-4488-97fb-a6ccb846c87f": "How has the rapid progress in AI been achieved in the last decade, and what industries are currently utilizing AI algorithms?",
        "42f4e412-eff7-4b38-8fb5-8c84f774a8e7": "In what three broad areas does the author suggest that the deployment of AI technologies may have economic and social costs if not properly regulated?",
        "3bbce59e-9498-4e67-b783-8cea245072ac": "How does the demand for AI technologies for data potentially lead to privacy violations, unfair competition, and behavioral manipulation in the marketplace? Provide examples to support your answer.",
        "e913ab1a-f4cd-4857-ab2e-f236b7ea9750": "Discuss the labor market effects of AI, focusing on the potential exacerbation of efforts to reduce labor costs through automation and the impact on workers. How does the deployment of AI technologies contribute to this trend, and what are the potential consequences for the economy and society?",
        "687e9c0b-cfca-4a4f-b293-95de7e7cb24a": "How does the increasing use of AI in decision-making processes potentially impact the decision-making abilities of humans, especially in terms of specialization and economies of scope across tasks? Provide examples to support your answer.",
        "52738215-8d3c-4778-8461-7bccae8081a9": "Discuss the potential negative effects of AI on democratic politics, focusing on echo chambers in social media, problems of online communication, big brother effects, and the impact of automation on democracy. How do these effects contribute to the impairment of democratic discourse, and what are the common roots of these concerns?",
        "795ba67a-09a7-4c49-91a6-625ca2b74765": "How does the author suggest that AI exacerbates existing societal problems related to corporations, automation, labor relations, and democracy?",
        "27898e41-dc57-4668-b44b-54c6693d0b55": "According to the author, what factors contribute to the potential negative effects of AI, and why is it important to consider the costs and direction of development of this technology?",
        "b431c5e0-9a9c-435b-a666-de625807ccbe": "How do AI technologies rely on data for their functioning, and what is the prevailing approach in the field? Discuss the potential positive effects of data on prediction, product design, and innovation according to AI researchers and economists.",
        "1066007f-cd78-47de-967c-545bde93b7e4": "In what ways can data and information be misused in the context of AI, as highlighted by legal scholars and social scientists? Provide examples of how data exploitation can benefit digital platforms and tech companies at the expense of consumers and workers.",
        "b2485006-74d1-4878-8784-330d779658a0": "How does the social dimension of data impact the costs and benefits of data, according to the research discussed in the text? Provide examples to support your answer.",
        "5682702c-04e2-40b5-ae9a-da7de48843f1": "Explain the concept of data externalities and submodularity as discussed by Acemoglu et al. (2021) in relation to the sharing of personal data. How do these concepts influence the value of individuals' data and potential data buyers?",
        "109d6fa2-806f-4375-95b9-85ec80e3566a": "Explain the concept of leaked information about a user in the context of the model presented by Acemoglu et al. (2021). How is leaked information quantified and why is it important for the platform?",
        "ff4224d3-63a6-4e3f-b878-cdd52af18108": "Compare and contrast the objectives of the platform and the individual users in the model. How do the platform and users prioritize data-sharing, privacy protection, and financial incentives in their decision-making processes?",
        "6c344c19-e14f-41f0-84c7-ef09e9a39e70": "Explain the concept of submodularity in the context of data sharing and welfare as discussed in the provided text. How does submodularity impact the information transmitted by an individual sharing their data and the overall social benefits of data transactions?",
        "b161d992-e309-4247-b877-af8e9c441924": "Using the example of a platform with two users, where one user has a small value of privacy (v1 < 1) and the other has a large value of privacy (v2 > 1), discuss how data externalities can lead to inefficiency in data transactions. How does the correlation coefficient between the two users' data play a role in determining the social optimality of data sharing in this scenario?",
        "c04602ef-341a-4877-8a6e-12fde69310ad": "Explain how the concept of data leakage between two users with highly correlated information can lead to a situation where data prices become close to zero in a data market. How does submodularity play a role in this scenario?",
        "466d4df3-72e5-4885-849c-133dba044cd0": "Discuss the implications of the externalities and submodularity on data transactions and the distribution of surplus in a data market, using examples from the provided context information. How do these factors influence the willingness of users to protect their data and the overall efficiency of the market?",
        "5855b89c-622c-439c-8e96-364277b61048": "How do data externalities impact the efficiency of markets, particularly in the context of digital platforms with monopoly or quasi-monopoly situations? Discuss the potential costs and benefits associated with the use of data by corporations and platforms.",
        "2e5beb9c-36d5-4dcb-9d88-0cf97b9642a7": "Explain the concept of submodularity in the context of data privacy and its implications for individual willingness to protect their data. How does this factor contribute to the distributional consequences of data use and AI technologies?",
        "465a6fb7-07fd-43d4-9b76-c715ff46f88e": "Explain how AI technologies can impact competition in the marketplace, specifically in terms of data collection and use. Provide examples to support your explanation.",
        "d4f8a863-c380-4f93-b8ba-6562aea09300": "In the context of the Hotelling-type static model with two firms, describe the process of competition between the firms before the introduction of AI. How does the introduction of AI technologies change the dynamics of competition in this model?",
        "f5bc09d2-23f9-41a5-98d1-e7e47aae75b3": "Explain the concept of maximal product differentiation in the context of the model discussed in the passage. How does this concept relate to the equilibrium reached by the firms in the pre-AI environment?",
        "f67bf93b-e110-4f58-82c6-f5c27380b4eb": "In the post-AI environment described in the passage, how does the introduction of AI technology affect the strategies of the two firms? Discuss the changes in product offerings, pricing strategies, and equilibrium outcomes for both firms in this new setting.",
        "284f1e3b-0e7a-48cd-a015-e74ec5ebf80d": "Explain the impact of AI technologies on consumer surplus as discussed in the provided context. How does the use of detailed consumer data for prediction affect consumer welfare and firm profits?",
        "0d578372-2a40-4439-a674-b1f20e755753": "Discuss the implications of AI technologies and detailed consumer data on market dynamics, particularly in terms of customization of products and distribution of consumer surplus. How does the model presented in the context highlight the potential trade-offs between consumer welfare and firm profitability in AI-intensive environments?",
        "da3cdd27-91cc-483b-b93f-1d0354dd5323": "How does the better collection and processing of data by firms impact price competition in the market, and what are the potential distributional effects of this phenomenon?",
        "75fc2241-1523-41d3-9eec-07bf84f2d95d": "Discuss the potential negative effects of platforms using AI technologies and massive data sets for behavioral manipulation, providing examples and explaining how consumers may be affected.",
        "bdef3348-5f7a-48e9-acd7-bb0f650cbc83": "Explain the concept of prior beliefs and signals in the dynamic setting described in the context. How do these factors influence consumer behavior and the platform's pricing strategy?",
        "0fc4242c-25b4-4dcb-b66d-bb847e1ab2a2": "Discuss the implications of the pre-AI equilibrium strategy adopted by the platform in the given scenario. How does the platform's pricing and product offering decisions change based on the consumer's experience and the signals received after consumption?",
        "10924804-2a5c-4c51-953b-1f87e99d0300": "Explain how the deployment of AI technologies impacts the platform's ability to predict consumer preferences and behavior. How does this affect the consumer's decision-making process in the post-AI world?",
        "f18ea29a-e45a-4af1-8ab0-50660629a56e": "In the post-AI world, how does the platform's knowledge of the event \u0018j i impact consumer behavior and the equilibrium in the market? Provide an example to illustrate this concept.",
        "feaf3e75-6a85-498c-b029-98be21e05e04": "Explain the rationale behind the platform's decision to offer product 2 as the initial offering, despite potentially sacrificing some revenue in the first period. How does this strategy impact the platform's profits and consumer welfare?",
        "84bea162-4945-483f-9d54-38ee5eae15ee": "Analyze the consumer welfare implications of the platform's two strategies for offering products. How does the consumer's valuation and surplus differ between the two strategies, and what factors contribute to these differences?",
        "3affde2a-3a0c-4f7f-8db7-f4ef8779e16e": "How can AI technologies enable platforms to manipulate consumer behavior, leading to potential distortions in consumption patterns and reduced utilitarian welfare? Provide examples from the text to support your answer.",
        "49a7ec1e-ee6f-4313-8ced-8ad29d2cc78c": "Discuss the labor market effects of AI as outlined in the document. How have US labor markets been impacted over the last 40 years, particularly in terms of wage growth and income distribution? Use specific data and trends mentioned in the text to support your analysis.",
        "ed8ea40f-1338-46de-be28-c80689e53784": "How has the decline in the power of trade unions, increased trade with China, and changes in technological progress contributed to the reshaping of the US wage structure between 1980 and 2016?",
        "43394af3-d137-4c25-b404-58efb13799ec": "Discuss the potential impact of AI on the labor market, considering its potential for automation, generation of new tasks, and monitoring of workers. How might AI exacerbate existing concerns about excessive automation and wage disparities?",
        "7ce63d78-dafe-4d38-bdbc-3dedf6cc3f52": "Explain the framework proposed by Acemoglu and Restrepo for modeling the automation of tasks and the role of AI in the economy. How do factors such as capital, labor, and task-specific productivities play a role in this framework?",
        "6cfbb017-d20c-4b92-983f-0a6b0b4759e5": "Discuss the concept of competitive equilibrium in the economy as characterized by Acemoglu and Restrepo. How do factors such as wage floors, labor market regulations, and labor market imperfections impact the equilibrium wage in the context of excessive automation and AI?",
        "026c1172-ff1b-41c2-a2c9-ce0d6604f23a": "Explain the concept of the displacement effect in the context of automation and its impact on the labor market equilibrium. How does the displacement effect contribute to the decline in the labor share and potential decrease in wage levels?",
        "a4bdac9c-1095-41ac-a69c-b58ddd0470e9": "Discuss the relationship between automation, productivity, and employment in the labor market equilibrium. How does the presence of a wage floor affect the impact of automation on employment levels, and what role does the productivity effect play in determining wage outcomes?",
        "8284b8e2-dd9b-49ce-afc3-398b820797c6": "How does the presence of labor market imperfections, such as a wage floor, impact the welfare consequences of employment-reducing automation? Provide an explanation for why excessive automation may occur in such circumstances.",
        "2bcae304-e4d4-4438-9bf4-d99337239562": "Discuss the relationship between productivity effects and the likelihood of excessive automation. How does the level of productivity gains from automation influence the overall welfare implications of automation on employment and net output?",
        "f4fdbcb5-707d-460c-88c3-4ae71638ad7f": "How does automation, particularly through the use of AI technologies, impact the labor market in terms of the labor share, wages, and employment levels? Provide examples from the text to support your answer.",
        "3b1a8d8c-ec7f-49a1-bc1d-60facad09c10": "Discuss the potential welfare implications of AI technologies being deployed for automation, considering factors such as productivity gains, task displacement, and the social opportunity cost of labor. How do these factors contribute to the overall impact on society and the economy?",
        "50dbe296-98f2-4935-8524-01b963404524": "How does the framework presented in the document suggest that the use of AI for creating new tasks can have a positive impact on social welfare, particularly in terms of employment and wages? Provide a detailed explanation of the productivity effect and reinstatement effect in this context.",
        "99e56b80-0750-4ecc-8f49-a4e7a6c2524d": "According to Acemoglu and Restrepo (2019), what role did the introduction of new tasks play in counterbalancing the labor market implications of automation in the decades following World War II? How does this argument support the idea that using AI for new tasks could be welfare-improving, especially in the presence of labor market imperfections?",
        "f6bb3dfa-e0e4-4aac-b5ac-14b1eba1c33d": "How do labor market imperfections and the role of factor prices influence the balance between automation and new tasks in the context of AI technology, as discussed by Acemoglu and Restrepo (2018)?",
        "c7e16ffa-0f21-4d67-b818-98d5c833cf67": "Discuss the potential distortions in the direction of technological change related to the business models of leading firms and the tax code, as outlined in the readings by Acemoglu (2021) and Acemoglu, Manera, and Restrepo (2020).",
        "ff85c26d-a6f0-4e62-a0aa-4a5acc589569": "How does the focus on achieving \"human parity\" in narrow tasks impact the direction of technological change towards automation, and what potential harmful effects on the labor market could result from this trend?",
        "7a5d1f30-b437-41a8-8051-06870326e757": "Discuss the potential benefits and drawbacks of using AI for increasing worker productivity and expanding the set of tasks in which humans have a comparative advantage, as opposed to solely focusing on automation.",
        "242c36fe-0a5e-4e1c-9e62-3bc1596eb0a1": "Explain the concept of economies of scope in the context of AI advancements and its impact on net output in the economy. How does the presence of economies of scope affect the allocation of tasks between workers and AI algorithms?",
        "cf0c6974-e3ac-491e-9d52-180acc080b7d": "Discuss the conditions under which the adoption of AI algorithms would lead to an increase in net output in the economy, considering factors such as the cost of AI technology, economies of scope, and the productivity of workers in different tasks. How do these factors interact to determine the overall output level in the economy post-AI adoption?",
        "00d54789-c4a4-4d06-ba49-f2e727dc2343": "How does the delegation of certain tasks to AI technologies impact human workers' ability to understand the holistic aspects of relevant tasks and potentially reduce their productivity? Provide an example from the text to support your answer.",
        "0502ae94-9171-4203-ac2a-941156479c33": "Discuss the potential market implications of adopting AI technologies in terms of economies of scope and the finer division of labor. How might the entry of new firms specializing in AI technologies disrupt the pre-existing equilibrium and drive the economy towards a post-AI equilibrium, even if it is inefficient? Use examples from the text to illustrate your points.",
        "5b53c728-3ea0-4e20-a696-78130866a02d": "How does the use of AI technologies in worker monitoring potentially lead to inefficiencies in the allocation of resources between employers and employees, according to the model based on Acemoglu and Newman (2002)? Provide a detailed explanation of the concept of monitoring and its impact on the distribution of rents in the economy.",
        "5559ce21-a69d-49b7-8028-fdaab034430b": "Discuss the potential implications of cost-minimization incentives for firms in utilizing AI technologies inefficiently, particularly in the context of economies of scope. How might the adoption of AI technologies affect the balance between human judgment and creativity in the workforce, and what are the potential consequences for productivity and efficiency?",
        "f0c65f62-7735-426c-93a6-8d857c20faac": "Explain the concept of the participation constraint in the context of the firm maximizing its profits while respecting the worker's ex ante reservation utility. How does this constraint impact the equilibrium in the model presented by Acemoglu and Newman (2002)?",
        "4e73f374-ecab-4f60-9702-15a078db41dd": "Discuss the implications of AI on monitoring levels and welfare in the equilibrium described in the framework. How does the removal of an upper bound on monitoring due to AI potentially lead to higher levels of rent-shifting activities by employers, and what are the potential consequences for social welfare and income inequality?",
        "4da0c7a7-67b0-4235-9e8d-7d7c1014ca55": "How do AI technologies impact the distribution of wages in the labor market, according to the information provided? Discuss the potential implications of AI on monitoring workers and its effect on efficiency wages.",
        "77651fee-6d00-4046-a58d-af3c5a64267c": "In what ways has the political landscape changed in recent years, as discussed in the document? Explain the concept of echo chambers and polarization in the context of social media and its potential impact on democratic discourse.",
        "cca67ba4-4175-4428-bb00-ee67667d01d0": "How does the concept of echo chambers, as discussed in the provided text, contribute to the fragmentation and extremism within society? Provide examples from the text to support your answer.",
        "0081a9a3-e72d-4c0f-b057-0ee08c10aadb": "Explain the role of AI algorithms in creating filter bubbles on social media platforms, as outlined in the document. How do these filter bubbles contribute to the spread of misinformation and the reinforcement of individuals' biases? Use relevant studies mentioned in the text to support your explanation.",
        "7e740d1a-9be8-4c2e-9f11-e377a1d6729a": "How does the level of homophily in a community affect the likelihood of a news item becoming viral, according to the information provided?",
        "c3c1c4b6-ff7e-4c2d-8608-6c7c121090c4": "Explain how the suspicion levels of individuals in a community influence their likelihood of inspecting news items, based on their prior beliefs and the source of the message.",
        "de6af4cb-d006-46f3-af2a-5cb56ef39e49": "How does the platform's choice of algorithms influence the degree of homophily, and what is the platform's objective in maximizing engagement? Provide examples to support your answer.",
        "44cf4e5f-7185-46f0-a1d5-f5638bc42935": "Discuss the implications of low-reliability news items and the distribution of news within polarized communities on the platform's engagement-maximizing policy. How does the platform's use of algorithms contribute to the propagation of misinformation in this context?",
        "39d218a2-1e6a-4c27-bd41-054c78158a4c": "Explain the concept of echo chambers and filter bubbles in the context of AI-powered social media platforms. How do these phenomena impact individuals' willingness to share information and contribute to the circulation of misinformation?",
        "97879b67-7368-415e-a6ef-0dd0f06e7a2c": "Discuss the potential perils of online communication for political discourse, as outlined in the document. How does the nature of online communication, particularly in the context of social media platforms, create challenges for effective political communication and information sharing?",
        "5bcadecf-74c4-426b-852f-8e1dc2d94a53": "Explain the concept of \"channel constraints\" in the context of the scenario described. How do these constraints impact the individual's ability to share political news or gossip with their neighbor?",
        "d59fadf1-8215-454d-b33b-c42babeb12dd": "Discuss the role of ulterior motives and extremist types in influencing individuals' beliefs in the scenario. How does the probability of an individual attaching to their neighbor being an extremist affect the spread of political news in the social network?",
        "0e178987-a3a9-4435-8e9f-f883f27e89d6": "How does the shift to online communication impact the probability of individuals attaching to extremist views, and what implications does this have for news exchange and gossip within a network?",
        "5314c934-6e45-422a-951c-922eedfde1e8": "Discuss the challenges that online communication, particularly in the form of broadcast communication, poses for political discourse and democracy, as outlined in the provided context information.",
        "00f6f678-1c66-4c14-a10f-116bfbbef2fa": "How does the use of AI technologies in monitoring communication and political activity potentially impact the ability of opposition groups to organize protests and civil disobedience in both democratic and nondemocratic regimes?",
        "ed07a2c8-4c73-427b-9e36-0ee507d6b479": "Using the model outlined in the text, explain how the heterogeneity in the cost of participating in protest activities among citizens can influence the effectiveness of bottom-up pressures for social and political change in a society.",
        "f5ddcbd7-b010-4216-9eef-c95a4feebc26": "Explain the political structure described in the context, focusing on the interactions between the elite's policy choices, citizen protests, and the probability of policy change. How does the punishment imposed on protesters by the government factor into this political game?",
        "f01bc37b-cf38-4d84-8208-351b0d0603e9": "Analyze the utility functions of both citizens and elites as outlined in the context. How do the citizens' utility from protesting and the elites' utility from policy choices differ? Discuss the threshold value \u0016c and its significance in determining citizen participation in protests.",
        "0a3caee0-c64b-4f1a-bbf4-2c080db57c05": "Explain the concept of subgame perfect equilibrium in the context of the model presented in the passage. How does backward induction play a role in determining the equilibrium?",
        "ce1fe31e-b2de-47d2-b6d9-a57c9aa2dc41": "How does the introduction of AI, modeled as an increase in a certain parameter, impact the behavior of elites and citizens in the model? Discuss the implications of AI on the responsiveness of policies to citizens' wishes and the dynamics of protests in the society.",
        "d38f3cfd-18a5-4721-b40e-118bfcb1b0fe": "How can AI technologies be utilized for government monitoring against protest activities, and what implications does this have for democracy and policy distortions according to the model presented in the text?",
        "f1f3fc82-21b7-490e-ae4e-cd14bef29b45": "Discuss the relationship between automation, workers' political power, and democracy as outlined in the subsection. How does automation potentially weaken workers' political power and what implications does this have for democratic institutions?",
        "9cbeed63-b268-4d92-b7b7-ceeda8017e93": "Explain the relationship between automation and output in the context of competitive labor markets. How does the competitive rental rate of capital compare to the wage in determining the cost-saving nature of automation?",
        "09aeaab0-b32c-4a1f-8047-7eaf0e6babfe": "In the scenario described, analyze the decision-making process of the elite (capitalists) in a political system where they have power over workers. Discuss the implications of redistributive politics, worker aspirations, and labor productivity on the elite's payoffs and decision to meet worker demands.",
        "c3c5acf2-aa35-4f0d-a72f-aa1c870c009e": "Explain the impact of automation on the comparison of strategies related to labor cooperation and redistribution as discussed in the provided text. How does automation influence the decision-making process of elites in terms of democratic concessions and inequality?",
        "4a7bc943-e44c-4ce7-9de1-12237e1acf77": "Discuss the implications of low-productivity automation on the preference for strategies regarding labor cooperation and redistribution. How does the threshold level of automation intensity affect the elite's stance towards democracy and redistributive politics?",
        "2e9decf0-c86a-43a2-9b24-53d9c4adfbcb": "How might the productivity benefits of automation impact labor cooperation and the potential consequences for democracy, redistribution, and social cohesion?",
        "2e854e77-49fc-4a6e-9c81-9234f21ec3ed": "Discuss the potential implications of the threat of AI on wages, inequality, and bargaining power, as outlined in the document.",
        "07b79be5-07bb-40b8-b377-161f822c8713": "Discuss the potential ethical and social dilemmas associated with the incorporation of AI technologies into weapons, particularly autonomous weapon systems. How might these technologies impact governments, civil society, protesters, and opposition groups?",
        "55fc495a-fbff-48a9-91dd-e3f37e43bd8b": "How does the \"alignment problem\" pose a potential downside to AI technologies, and what measures can be taken to ensure that intelligent machines have objectives aligned with those of humanity? Additionally, how do concerns about super-human capabilities of machines relate to shorter-term problems created by AI technologies?",
        "12c86618-bbed-48cd-a52f-a91e68745363": "How do the potential costs of AI technologies discussed in the document depend on corporate and societal choices rather than the inherent nature of AI technologies themselves? Provide examples to support your answer.",
        "dc9e865c-7dd8-4145-aca2-24e716b1c001": "Discuss the importance of regulation in addressing the harms caused by the misuse of data by AI technologies, as outlined in the document. How can regulations on data control rights help prevent or minimize the negative effects on consumers and privacy?",
        "29c79b98-049b-4211-83dd-6e2260fa8771": "How can informational warnings about data usage help prevent potential harms to consumers, according to the text? Discuss the effectiveness of such warnings and provide examples of how they could be implemented.",
        "1e30acbf-2ad6-424d-aee2-1b239d8988df": "In what ways does the text suggest that increasing competition may not always be an effective solution to addressing issues related to data misuse and consumer exploitation? Explain the nuances of competition in the context of firms using AI technology and acquiring consumer information.",
        "d8961c2c-df1d-4b56-9c6a-6a3a91490939": "How does the demand for AI technologies from corporations potentially impact the allocation of research efforts and the social and economic costs associated with innovation? Discuss the potential consequences of unregulated innovation in the context of AI development.",
        "d990e4f0-cc31-4ffa-9bec-fe8c2a9735ac": "In what ways can regulatory solutions address the challenges posed by excessive automation and the potential misuse of AI technologies in labor markets? How might distinguishing between different uses of AI, such as creating new tasks versus automating low-skill tasks, present measurement challenges for regulators and researchers?",
        "7bcc2bfe-5453-4bc8-9460-31bdb85c1279": "How does the author propose the implementation of a \"precautionary regulatory principle\" in the deployment of AI technologies, especially in domains such as political discourse and democratic politics? What potential benefits does the author suggest this principle could bring?",
        "8a209704-17ce-489f-9b68-8bf9c77468dd": "According to the author, what are some of the potential economic, political, and social costs associated with the current trajectory of AI technologies? How does the author emphasize the importance of studying and understanding these potential costs, and what policy responses does the author suggest to mitigate them?",
        "aa21c40b-4896-4afe-bdcd-544f263b03bc": "How does the author suggest that regulation of AI may be more challenging than regulation of other technologies, and what potential consequences could arise from inadequate regulation?",
        "018cb267-cae5-40c4-bf39-f3429eae3368": "In what ways does the author argue for the importance of starting conversations about potential harms of new technologies, despite historical examples of opposition from governments and powerful interest groups?",
        "493e5b47-ddb3-4c51-8779-a9774be9db9a": "In Acemoglu's research on AI and jobs, what evidence did they find from online vacancies regarding the impact of AI on employment?",
        "44365875-fe39-417c-81c3-46025462cf05": "According to Acemoglu and Simon Johnson's book manuscript, \"Men of Genius: How Hubris Ruined Technology and Prosperity,\" what key concept do they discuss in relation to technology and prosperity?",
        "578299e4-751f-4540-8b7c-68c453c69267": "In the study by Acemoglu and Robinson (2012), \"Why Nations Fail: The Origins of Power, Prosperity, and Poverty,\" what are the key factors identified as influencing the fate of nations in terms of power, prosperity, and poverty?",
        "2879b3e1-0d21-40f0-ac87-1220e3224a0c": "How do Acemoglu and Restrepo (2019) discuss the implications of automation on labor demand and wage inequality in the United States in their paper \"Automation and New Tasks: How Technology Changes Labor Demand\"?",
        "9e4818e4-cb5b-439e-be6c-94516249e839": "In what ways do Bergemann, Bonatti, and Gan discuss the concept of \"Markets for Information\" in their Yale mimeo paper, and how does it relate to the broader economic landscape?",
        "1f791d5d-fafe-4ccb-9531-a2ae15ca853c": "How do Jones and Tonetti explore the concept of \"Non-rivalry in the Economics of Data\" in their American Economic Review article, and what implications does this have for the understanding of data in the modern economy?",
        "95acb430-91e8-4524-a438-e6de6e267185": "In the study by Kleinberg et al. (2018), what were the key findings regarding human decisions and machine predictions, and how do they impact economic decision-making?",
        "724e81a3-abf2-41af-a06e-c667495421c7": "How does Roee Levy's research on social media, news consumption, and polarization contribute to our understanding of the effects of social media on society, and what implications does it have for public discourse and political polarization?",
        "f1d6e1e8-e951-497d-994a-eac8f7aaec82": "In Michael J. Sandel's book \"The Tyranny of Merit,\" what is the central argument regarding the common good and meritocracy?",
        "ddb671ff-22ef-474e-9325-cd2fc63e334d": "How does Tom Simonite's article \"Algorithms Were Supposed to Fix the Bail System. They Haven't\" highlight the limitations of using algorithms in the criminal justice system?"
    },
    "corpus": {
        "0443b8fb-2f8b-4df4-8a21-ff0e570298cf": "NBER WORKING PAPER SERIES\nHARMS OF AI\nDaron Acemoglu\nWorking Paper 29247\nhttp://www.nber.org/papers/w29247\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nSeptember 2021\nPrepared for The Oxford Handbook of AI Governance. I am grateful to many co-authors who \nhave contributed to my thinking on these topics and on whose work I have heavily relied in this \nessay. They include: David Autor, Jonathon Hazell, Simon Johnson, Jon Kleinberg, Anton \nKorniek, Azarakhsh Malekian, Ali Makhdoumi, Andrea Manera, Sendhil Mullainathan, Andrew \nNewman, Asu Ozdaglar,  Pascual Restrepo and James Siderius. I am grateful to David Autor, \nLauren Fahey, Vincent Rollet,  James Siderius and Glen Weyl for comments. I gratefully \nacknowledge financial support from Google, the Hewlett Foundation, the NSF, the Sloan \nFoundation, the Smith Richardson Foundation, and the Schmidt Sciences Foundation. The views \nexpressed herein are those of the author and do not necessarily  reflect the views of the National \nBureau of Economic Research.\nNBER working papers are circulated for discussion and comment purposes. They have not been \npeer-reviewed or been subject to the review by the NBER Board of Directors that accompanies \nofficial NBER publications.\n\u00a9 2021 by Daron Acemoglu. All rights reserved. Short sections of text, not to exceed two \nparagraphs, may be quoted without explicit permission provided that full credit, including \u00a9 \nnotice, is given to  the source.",
        "5cab1b9a-08f6-4d37-9b8d-4db148433719": "Harms of AI\nDaron Acemoglu\nNBER Working Paper No. 29247\nSeptember 2021\nJEL No. J23,J31,L13,L40,O33,P16\nABSTRACT\nThis essay discusses several potential economic, political and social costs of the current path of \nAI technologies. I argue that if AI continues to be deployed along its current trajectory and \nremains unregulated,  it may produce various social, economic and political harms. These include: \ndamaging competition,  consumer privacy and consumer choice; excessively automating work, \nfueling inequality, inefficiently  pushing down wages, and failing to improve worker productivity; \nand damaging political discourse, democracy's most fundamental lifeblood. Although there is no \nconclusive evidence suggesting that  these costs are imminent or substantial, it may be useful to \nunderstand them before they are fully realized  and become harder or even impossible to reverse, \nprecisely because of AI's promising and wide-reaching  potential. I also suggest that these costs \nare not inherent to the nature of AI technologies, but are related  to how they are being used and \ndeveloped at the moment - to empower corporations and governments against workers and \ncitizens. As a result, efforts to limit and reverse these costs may need to rely on regulation and \npolicies to redirect AI research. Attempts to contain them just by promoting competition  may be \ninsufficient.\nDaron Acemoglu\nDepartment of Economics, E52-446\nMassachusetts Institute of Technology\n77 Massachusetts Avenue\nCambridge, MA 02139\nand NBER\ndaron@mit.edu",
        "c6750a30-151b-499b-8592-0c25f25ade03": "1 Introduction\nTo many commentators, arti\u0085cial intelligence (AI) is the most exciting technology of our age,\npromising the development of \u0093intelligent machines\u0094that can surpass humans in various tasks,\ncreate new products, services and capabilities, and even build machines that can improve\nthemselves, perhaps eventually beyond all human capabilities. The last decade has witnessed\nrapid progress in AI, based on the application of modern machine learning techniques and\nhuge amounts of computational power to massive, often unstructured data sets (e.g., Russell\nand Norvig, 2009, Neapolitan and Jiang, 2018, Russell, 2019).1AI algorithms are now used\nby almost all online platforms and in industries that range from manufacturing to health,\n\u0085nance, wholesale and retail (e.g., Ford, 2015; Agarwal, Gans and Goldfarb, 2018; West,\n2018). Government agencies have also started relying on AI, especially in the criminal justice\nsystem and in customs and immigration control (Thompson, 2019; Simonite, 2020).\nWhether AI will be everything its enthusiastic creators and boosters dream, it is likely\nto have transformative e\u00a4ects on the economy, society and politics in the decades to come,\nand some of these are already visible in AI algorithms\u0092impact on social media, data markets,\nmonitoring of workers and work automation. Like many technological platforms (or \u0093general\npurpose technologies\u0094 ) that can be used for the development of a variety of new products,\nservices, and production techniques, there are a lot of choices about how AI technologies will\nbe developed. This, combined with the pervasive e\u00a4ects of AI throughout society, makes it\nparticularly important that we consider its potential dark side as well.\nIn this essay, I will focus on three broad areas in which the deployment of AI technologies\nmay have economic and social costs \u0097 if not properly regulated. I want to emphasize at\nthe outset that the arguments I will present are theoretical , and currently there is insu\u00a2 cient\nempirical evidence to determine whether the mechanisms I isolate are important in practice.\nThe spirit of the exercise is to understand the potential harms unregulated AI may create so\nthat we have a better understanding of how we should track and regulate its progress.\nThe areas I will focus on are:\n1.Collection and control of information . I will argue that the combination of the\n1The \u0085eld of \u0093AI\u0094 today is dominated by the suite of current arti\u0085cial intelligence technologies and ap-\nproaches, mostly based on statistical pattern recognition, machine learning and big data methods. The poten-\ntial harms of AI I discuss in this paper are relevant for and motivated by these approaches. Nevertheless, I will\nalso emphasize that \u0093AI\u0094 should be thought of as a broad technological platform, precisely because the gen-\neral aspiration to produce \u0093machine intelligence\u0094includes e\u00a4orts to improve machines in order to complement\nhumans, create new tasks and services, and generate novel communication and collaboration possibilities.\n1",
        "98a0ca4d-bc44-4bfe-b67d-b50e3554927c": "demand of AI technologies for data and the ability of AI techniques for processing vast\namounts of data about users, consumers and citizens produces a number of potentially\ntroubling downsides. These include: (a) privacy violation: companies and platforms may\ncollect and deploy excessive amounts of information about individuals, enabling them to\ncapture more of the consumer surplus via price discrimination or violate their privacy in\nprocessing and using their data; (b) unfair competition : companies with more data may\ngain a strong advantage relative to their competitors, which both enables them to extract\nmore surplus from consumers and also relaxes price competition in the marketplace, with\npotentially deleterious e\u00a4ects; and (c) behavioral manipulation: data and sophisticated\nmachine learning techniques may enable companies to identify and exploit biases and\nvulnerabilities that consumers themselves do not recognize, thus pushing consumers to\nlower levels of utility and simultaneously distorting the composition of products in the\nmarket.\n2.Labor market e\u00a4ects of AI . I will argue that even before AI there was too much\ninvestment in cutting labor costs and wages in the US (and arguably in some other\nadvanced economies as well). Such e\u00a4orts may be excessive either because, in attempting\nto cut costs, they reduce production e\u00a2 ciency, or because they create non-market e\u00a4ects\n(for example, on workers losing their jobs or being forced to take lower-pay work). AI, as\na broad technological platform, could have in principle recti\u0085ed this trend, for example,\nby promoting the creation of new labor-intensive tasks or by providing tools for workers to\nhave greater initiative. This does not seem to have taken place. For example, automation\nis a quintessential example of e\u00a4orts to cut labor costs, and like other e\u00a4orts, it may be\nexcessive. Many current uses of AI involve automation of work or the deployment of\nAI in order to improve monitoring and keep wages low, motivating my belief that AI\nmay be exacerbating the excessive e\u00a4orts to reduce labor costs. In this domain, I focus\non four broad areas: (a) automation : I explain why automation, a powerful way to\nreduce labor costs, can be part of the natural growth process of an economy, but it\ncan also be excessive, because \u0085rms do not take into account the negative impact of\nautomation on workers; (b) composition of technology : problems of excessive automation\nintensify when \u0085rms have a choice between investing in automation versus new tasks,\nand I explain why this margin of technology choice may be severely distorted, and how\nAI-type technologies may further distort this composition; (c) loss of economies of scope\nin human judgment : in contrast to the hope that AI will take over routine tasks and in\n2",
        "0b736938-6721-4587-a648-c0fb52a548b8": "the process enable humans to specialize in problem-solving and creative tasks, AI-human\ninterplay might gradually turn humans into worse decision-makers as they hand more and\nmore decisions to machines, especially when there are economies of scope across tasks;\nand (d) monitoring : I also explain how technologies like AI that increase the monitoring\nability of employers are very attractive to \u0085rms, but may at the same time generate\nsigni\u0085cant social ine\u00a2 ciencies.\n3.AI, communication and democracy. I will \u0085nally suggest that AI has also exac-\nerbated various political and social problems related to communication, persuasion and\ndemocratic politics that, once again, predate the onset of this technology. The main\nconcern here is that democratic politics may have become more di\u00a2 cult, or even funda-\nmentally \u0087 awed, under the shadow of AI. I focus on: (a) echo chambers in social media :\nhow AI-powered social media generates echo chambers that propagate false information\nand polarize society; (b) problems of online communication : I suggest that online social\nmedia, which is interwoven with AI, creates additional misalignments related to private\ncommunication; (c) big brother e\u00a4ects : AI increases the ability of governments to closely\nmonitor and stamp out dissent. All of these e\u00a4ects of AI are, by their nature, damaging,\nbut may have their most consequential e\u00a4ects by impairing democratic discourse; and\n(d)automation and democracy: \u0085nally, I suggest that the process of automation may\nfurther damage democracy by making workers less powerful and less indispensable in\nworkplaces.\nIn each one of the above instances, I discuss the basic ideas about potential costs and, when\nappropriate, I present some of the modeling details (in some cases based on existing work and in\nothers as ideas for future exploration). Throughout, my approach will be informal, attempting\nto communicate the main ideas rather than providing the full details of the relevant models.\nIn addition, I will point out the relevant context and evidence in some cases, even though,\nas already noted, we do not have su\u00a2 cient evidence to judge whether most of the mechanisms\nI am exploring here are likely to be important in practice. I then discuss the common aspects\nof the potential harms from AI and explore their common roots. I also argue that these costs,\nif proved important, cannot be avoided in an unregulated market. In fact, I will suggest that in\nmany of these instances, greater competition may exacerbate the problem rather than resolving\nit.\nThe aforementioned list leaves out several other concerns experts have expressed (AI leading\n3",
        "8aed6261-7d67-4901-9d2a-d762cf073805": "to evil super intelligence or AI\u0092 s e\u00a4ects on war and violence), mostly because of space restric-\ntions and also partly because they are further away from my area of expertise. I mention them\nbrie\u0087 y in Section 5.\nThe long list of mechanisms via which AI could have negative economic, political and social\ne\u00a4ects may create the impression that this technological platform is bound to have disastrous\nsocial consequences, or it may suggest that some of these problems are solely created by AI.\nNeither is true. Nor am I particularly opposed to this technology. I believe that AI is a hugely\npromising technological platform. Furthermore, with or without AI, our society has deep\nproblems related to the power of corporations, automation and labor relations, and polarization\nand democracy. AI exacerbates these problems, because it is a powerful technology and, owing\nto its general-purpose nature and ambition, it is applicable in a wide array of industries and\ndomains, which ampli\u0085es its ability to deepen existing fault lines. These qualities make the\npotential negative e\u00a4ects of AI quite di\u00a2 cult to foresee as well. Perhaps even more than\nwith other technologies and technological platforms, there are many di\u00a4erent directions, with\nhugely di\u00a4erent consequences, in which AI can be developed. This makes it doubly important\nto consider the costs that it might create. It also makes it vital to think about the direction\nof development of this technology.\nIndeed, my point throughout is that AI\u0092 s costs are avoidable, and if they were to tran-\nspire, this would be because of the choices made and the direction of research pursued by AI\nresearchers and tech companies. They would also be due to the lack of appropriate regulation\nby government agencies and societal pressure to discourage nefarious uses of the technology\nand to redirect research away from them. This last point is important: again like most other\ntechnologies, but only more so, the direction of research of AI will have major distributional\nconsequences and far-ranging implications for power, politics, and social status in society, and\nit would be na\u00efve to expect that unregulated markets would make the right trade-o\u00a4s about\nthese outcomes \u0097 especially since, at the moment the major decisions about the future of AI\nare being made by a very small group of top executives and engineers in a handful of com-\npanies. Put di\u00a4erently, AI\u0092 s harms are harms of unregulated AI. But in order to understand\nwhat needs to be regulated and what the socially optimal choices may be, we \u0085rst need to\nsystematically study what the downside of this technology may be. It is in this spirit that this\ncurrent essay is written.\nThe rest of this essay is organized as follows. In Section 2 I start with the e\u00a4ects that AI cre-\nates via the control of information. Section 3 moves to discuss AI\u0092 s labor market implications.\n4",
        "19e76f02-e07a-41e4-8990-e6c234f325d6": "Section 4 turns to the e\u00a4ects of this technological platform on social communication, polariza-\ntion and democratic politics. Section 5 brie\u0087 y touches upon a few other potential unintended\nconsequences of AI technologies. Section 6 steps back and discusses the role of choice in this\nprocess. I explain why the direction of technological change in general, and the direction of AI\nresearch in particular, is vital, and how we should think about it. This section also reiterates\nthat many of the costs mentioned in the preceding sections are the result of choices made\nabout the development and use of AI technologies in speci\u0085c directions. It then builds on the\nmechanisms discussed in previous sections to emphasize that unregulated markets are unlikely\nto internalize AI\u0092 s costs and how greater competition may sometimes make things worse, and\nnor are unfettered markets likely to direct technological change towards higher social value\nuses of AI. In this spirit, this section also provides some ideas about how to regulate the use\nof AI and the direction of AI research. Section 7 concludes.\n2 AI and Control of Information\nData are the lifeblood of AI. The currently-dominant approach in this area is based on turning\ndecision problems into prediction tasks and apply machine learning tools to very large data\nsets in order to perform these tasks. Hence, most AI researchers and economists working on AI\nand related technologies start from the premise that data create positive e\u00a4ects on prediction,\nproduct design and innovation (e.g., Brynjolfsson and McAfee, 2019; Jones and Tonetti, 2020;\nVarian, 2009; Farboodi et al., 2019). However, as emphasized by several legal scholars and\nsocial scientists, data and information can be misused \u0097 deployed in exploitative ways that\nbene\u0085t digital platforms and tech companies at the expense of consumers and workers (e.g.,\nPasquale, 2015, Zubo\u00a4, 2019). Zubo\u00a4, for example, argues that such exploitative use of data\nis at the root of the recent growth of the tech industry, which \u0093claims human experience as\nfree raw material for hidden commercial practice process of extraction, prediction, and sales.\u0094\n(2019, p. 8).\nIn this section, I discuss social costs of AI related to the control of data and information,\nwith a special emphasis on exploring when data can become a tool for excessive extraction and\nprediction.\n5",
        "686ca3f4-315b-41d7-a4a3-dd342f5715db": "2.1 Too Much Data\nConcerns about control and misuse of information become particularly important when there\nare bene\u0085ts to individuals from \u0093privacy\u0094 . Individuals may value privacy for instrumental or\nintrinsic reasons. The former includes their ability to enjoy greater consumer surplus, which\nmight be threatened if companies know more about their valuations and can charge them\nhigher prices. The latter includes various characteristics and behaviors that individuals would\nprefer not to reveal to others. This could be for reasons that are economic (e.g., to avoid\ntargeted ads), psychological (e.g., to maintain a degree of autonomy), social (e.g., to conceal\ncertain behaviors from acquaintances), or political (e.g., to avoid persecution).\nStandard economic analyses tend to view these privacy-related costs as second-order for\ntwo related reasons: if individuals are rational and are given decision rights, then they will\nonly allow their data to be used when they are compensated for it adequately, and this would\nensure that data will be used by companies only when their bene\u0085ts exceed the privacy costs\n(e.g., Varian, 2009; Jones and Tonetti, 2020). Secondly, in surveys individuals appear to be\nwilling to pay only little to protect their privacy, and hence the costs may be much smaller\nthan the bene\u0085ts of data (e.g., Athey et al., 2017). Yet these arguments have limited bite\nwhen the control of data has a \u0093social\u0094dimension \u0097 meaning that when an individual shares\nher data, she is also providing information about others. This social dimension is present, by\ndefault, in almost all applications of AI, since the use of data is speci\u0085cally targeted at learning\nfrom like-cases in order to generalize and apply the lessons to other settings.\nHow does this social dimension of data a\u00a4ect the costs and bene\u0085ts of data? This is\na question tackled in a series of papers, including MacCarthy (2010), Choi, Jeon and Kim\n(2019), Acemoglu et al. (2021) and Bergemann et al. (2021), and here I base my discussion on\nAcemoglu et al. (2021). The social dimension of data introduces two interrelated e\u00a4ects. First,\nthere will be data externalities \u0097 when an individual shares her data, she reveals information\nabout others. To the extent that data is socially valuable and individuals do not internalize\nthis, data externalities could be positive. But if indirect data revelation impacts the privacy\nof other individuals, these externalities could be negative. The second e\u00a4ect is what Acemoglu\net al. call submodularity : when an individual shares her data and reveals information about\nothers, this reduces the value of others\u0092information both to themselves and to potential data\nbuyers (such as platforms or AI companies). This is for the simple reason that when more\ninformation is shared about an individual, the less important the individual\u0092 s own data become\nfor predicting his or her decisions.\n6",
        "a7cee37b-c8ad-4be2-9088-064f34e21ac9": "Acemoglu et al. (2021) model this in the following fashion. Consider a community con-\nsisting ofnagents/users interacting on a (monopoly) digital platform. Each agent ihas a\ntype denoted by xiwhich is a realization of a random variable Xi, where the vector of random\nvariables X= (X1;:::;Xn)has a joint normal distribution N(0;\u0006), with covariance matrix\n\u00062Rn\u0002n(and \u0006ii=\u001b2\ni>0denoting the variance of individual i\u0092 s type). Each user has\nsome personal data, Si, which are informative about her type. Personal data include both\ncharacteristics that are the individual\u0092 s private information (unless she decides to share) and\nalso data that she generates via her activity online and o\u00a4-line. Suppose Si=Xi+Ziwhere\nZiis a normally-distributed independent random variable, Zi\u0018N(0;1).\nAlthough Acemoglu et al. (2021) discuss various metrics, here I suppose that the relevant\nnotion of information is mean square error (MSE). Then we can de\u0085ne leaked information\nabout user ias the reduction in the mean square error of the best estimator of her type:\nIi(a) =\u001b2\ni\u0000min\n^xiE\u0002\n(Xi\u0000^xi(Sa))2\u0003\n;\nwhere Sis the vector of data the platform acquires, ^xi(S)is the platform\u0092 s estimate of the\nuser\u0092 s type given this information, and a= (a1;:::;an)is the data-sharing action pro\u0085le of\nusers (with ai= 0denoting no direct data-sharing and ai= 1corresponding to data-sharing).\nThen, the objective of the platform is to maximize\nX\ni[\u0011Ii(a)\u0000aipi];\nwherepidenotes payment (\u0093price\u0094 ) to user ifrom the platform, which is made only when the\nindividual in question shares her data directly (i.e., ai= 1), and\u0011 >0. The price could take\nthe form of an actual payment for data shared or an indirect payment by the platform, for\nexample, the provision of some free service or customization. This speci\u0085cation embeds the\nidea that the platform would like to acquire data in order to better forecast the type/behavior\nof users.\nUseri\u0092 s objective is di\u00a4erent. She may wish to protect her privacy and she obviously\nbene\u0085ts from payments she receives. Thus her objective is to maximize:\n\rX\ni06=iIi0(a)\u0000viIi(a) +aipi:\nThe \u0085rst term represents any positive direct externalities from the information of other users\n(for example, because this improves the quality of services that the individual receives and\n7",
        "c1580fb4-9d05-4d96-ae0e-c77369b8f0c3": "does not fully pay for) and thus \r\u00150. The second term is the loss of privacy (capturing both\ninstrumental and intrinsic values of privacy). Hence vi\u00150here denotes the value of privacy\nto useri. Finally, the last term denotes the payments she receives from the platform.\nThis framework allows data to create positive or negative total bene\u0085ts. To illustrate this\npoint, suppose that vi=v. In that case, data create aggregate (utilitarian) bene\u0085ts provided\nthat\u0011+\r(n\u00001)>v. In contrast, if \u0011+\r(n\u00001)<v, the corporate control and use of data\nis socially wasteful (it creates more damage than good). But even in this case, as we will see,\nthere may be data transactions and extensive use of data. In general, because vidi\u00a4ers across\nagents, data about certain users may generate greater social bene\u0085ts than the costs, while the\nrevelation of data about others may be excessively costly.\nIn terms of market structure, the simplest option is to assume that the platform makes\ntake-it-or-leave-it o\u00a4ers to users in order to acquire their data.\nA key result proved in Acemoglu et al. (2021) is that Ii(a)is monotone and submodular.\nThe \u0085rst property means that when an individual directly shares her data, this weakly increases\nthe information that the platform has about all individuals, i.e., Ii(a0)\u0015Ii(a)whenever a0\u0015a.\nMathematically, the second implies that ,for two action pro\u0085les aanda0witha0\n\u0000i\u0015a\u0000i, we\nhave\nIi(ai= 1;a\u0000i)\u0000Ii(ai= 0;a\u0000i)\u0015Ii(ai= 1;a0\n\u0000i)\u0000Ii(ai= 0;a0\n\u0000i):\nEconomically, it means that the information transmitted by an individual directly sharing her\ndata is less when there is more data-sharing by others.\nI now illustrate the implications of this setup for data sharing and welfare using two simple\nexamples. Consider \u0085rst a platform with two users, i= 1,2, and suppose that \r= 0,\u0011= 1,\nandv1<1so that the \u0085rst user has a small value of privacy, but v2>1, implying that because\nof strong privacy concerns, it is socially bene\u0085cial not to have user 2\u0092 s data be shared with\nthe platform. Finally, suppose that the correlation coe\u00a2 cient between the data of the two\nusers is\u001a >0. Sincev1<1, the platform will always purchase user 1\u0092 s data. But this also\nimplies that it will indirectly learn about user 2 given the correlation between the two users\u0092\ndata. Ifv2is su\u00a2 ciently large, it is easy to see that it would be socially optimal to close o\u00a4\ndata transactions and not allow user 1 to sell her data either. This is because she is indirectly\nrevealing information about user 2, whose value of privacy is very large. This illustrates how\ndata externalities lead to ine\u00a2 ciency. In fact, if v2is su\u00a2 ciently large, the equilibrium, which\nalways involves user 1 selling her data, can be arbitrarily ine\u00a2 cient.\nMore interesting are the consequences of submodularity, which can be illustrated using this\n8",
        "168b5393-ee8f-4b57-b067-faeb991ba1e9": "example as well. To understand these, let us consider the edge case where the information of\nthe two users is very highly correlated, i.e., \u001a\u00191. In this example, the platform will know\nalmost everything relevant about user 2 from user 1\u0092 s data. The important observation is that\nthis data leakage about user 2 undermines the willingness of user 2 to protect her data. In\nfact, since user 1 is revealing almost everything about her, she would be willing to sell her own\ndata for a very low price. In this extreme case with \u001a\u00191, therefore, both the willingness of\nthe platform to buy user 2\u0092 s data and bene\u0085ts user 2 receives from protecting her data are\nvery small, and thus this price becomes approximately 0. But here comes the disturbing part\nfor data prices and the functioning of the data market in this instance: once the second user\nis selling her data, this also reveals the \u0085rst user\u0092 s data almost perfectly, so the \u0085rst user can\nonly charge a very low price for her data as well. As a result, the platform will be able to\nacquire both users\u0092data at approximately zero price. This price, obviously, does not re\u0087 ect\nusers\u0092value of privacy. They may both wish to protect their data and derive signi\u0085cant value\nfrom privacy. Nevertheless, the market will induce them to sell their data for close to zero\nprice. Imagine once again that v2is su\u00a2 ciently high. Then, despite this high value of privacy\nto one of the users, there will be a lot of data transactions, data prices will be near zero, and\nthe equilibrium will be signi\u0085cantly (arbitrarily) ine\u00a2 cient. These consequences follow from\nsubmodularity.\nAs a second example, consider the case in which again \r= 0and\u0011= 1but now there is\nno heterogeneity between the two users, so that v1=v2=v >1. This con\u0085guration implies\nthat neither user would like to sell their data (because their privacy is more important than\nthe value of data to the platform). Nevertheless, it can be shown that so long as vis less\nthan some threshold \u0016v(which is itself strictly greater than 1), there exists an equilibrium in\nwhich the platform buys the data of both users for relatively cheap. This too is a consequence\nof submodularity: when each user expects the other one to sell their data, they become less\nwilling to protect their own data and more willing to sell it for relatively cheap. This locks\nboth users into an equilibrium in which their data are less valuable than they would normally\nassume, and partly as a result, there is again too much data transaction.\nOne \u0085nal conclusion is worth noting. In addition to leading to excessive data use and\ntransactions, the externalities also shift the distribution of surplus in favor of the platform. To\nsee this, suppose v1=v2=v\u00141and\u001a\u00191, so that it is now socially optimal for data to\nbe used by the platform. It is straightforward to verify that in equilibrium, data prices will\nagain be equal to zero, and thus all of the bene\u0085ts from the use of data will be captured by\n9",
        "7ac6f958-31c4-46f3-929e-bdfedfa872bc": "the platform.\nAre data externalities and the ine\u00a2 ciencies they create empirically relevant? Like many\nof the channels I discuss in this essay, the answer is that we do not know for sure. If, as\nindustry insiders presume, bene\u0085ts of data are very large, then they will outweigh the costs\nfrom data externalities I have highlighted here. Even in this case, the market equilibrium will\nnot be fully e\u00a2 cient, though the use of data by platforms and corporations may be welfare-\nincreasing overall. However, there are reasons to believe that privacy considerations may be\nquite important in practice. First, many digital platforms have a monopoly or quasi-monopoly\nsituation (such as Google, Facebook or Amazon), and thus their ability to extract rents from\nconsumers can be signi\u0085cant. Second, some of the intrinsic reasons for consumers to care about\nprivacy \u0097 related to dissent and civil society activity \u0097 are becoming more important, as I\ndiscuss in Section 4.\nIn summary, the general lessons in this case are clear: when an individual\u0092 s data are relevant\nabout others\u0092behavior or preferences (which is the default case in almost all applications of\ndata), then there are new economic forces we have to take into account, and these can create\ncosts from the use of data-intensive AI technologies. In particular:\n1. The social nature of data \u0097 enabling companies to use an individual\u0092 s data for predicting\nothers\u0092behavior or preferences \u0097 creates externalities, which can be positive or negative.\nWhen negative externalities are important, there will tend to be too much use of data\nby corporations and platforms.\n2. The social nature of data additionally generates a new type of submodularity, making\neach individual less willing to protect their data when others are sharing theirs. This\nsubmodularity adds to the negative externalities, but even more importantly, it implies\nthat data prices will be depressed and will not re\u0087 ect users\u0092value of data and/or privacy.\n3. In addition to leading to excessive use of data, both of these economic forces have \u0085rst-\norder distributional consequences: they shift surplus from users to platforms and com-\npanies.\nIf these costs of data use and AI are important, they also call for regulating data markets.\nSome regulatory solutions are discussed in Acemoglu et al. (2021), and I return to a more\ngeneral discussion of regulation of AI technologies and data in Section 6.\n10",
        "8ea5e54b-3876-4404-80c6-125f69756557": "2.2 Data and Unfair Competition\nAI technologies amplify the ability of digital platforms and companies using data from these\nplatforms to predict consumer preferences and behavior. On the upside, this might enable\n\u0085rms to design better products for customers (after all, this is one of the main bene\u0085ts of AI).\nBut the use of such data can also change the nature of competition. These e\u00a4ects become even\nmore pronounced when some \u0085rms are much better placed to collect and use data relative to\ntheir competitors, and this is the case I will focus on this subsection. Speci\u0085cally, one \u0085rm\u0092 s\ncollection and use data that others cannot access may create a type of \u0093unfair competition\u0094 ,\nenabling this \u0085rm to capture consumer surplus and relax price competition. I now develop this\npoint in the simplest possible setting, using a Hotelling-type static model with two \u0085rms. The\nmain lesson will be that even when data improves product quality, it creates powerful forces\nthat shift the distribution of surplus away from consumers and towards \u0085rms.\nSuppose that consumers are located uniformly across a line of length 1 and incur a cost\n\u0097 similar to a transport cost \u0097 when they purchase a product further away from their bliss\npoint, represented by their location (see, for example, Tirole, 1989). I assume that the utility\nof consumer iwith location (or bliss point) ican be written as\n\u000b\u0000\f(xf\ni\u0000i)2\u0000pf\ni;\nwherexf\ni2[0;1]is the product of \u0085rm f2f0;1gandpf\niis its (potentially) customized price\nfor this consumer. Throughout, we normalize the cost of production to zero for both \u0085rms\n(regardless of whether they produce a standardized or customized product).\nLet us interpret the two \u0085rms as two di\u00a4erent websites, which consumers visit in order to\npurchase the good in question. Before AI, \u0085rms cannot observe the type of consumer and I\nassume that they cannot o\u00a4er several products to a consumer that visits their websites. Thus\nthey will have to o\u00a4er standardized products. This description implies that, in terms of timing,\nthey \u0085rst choose their product, and then after observing each other\u0092 s product choice, they set\nprices. Since each \u0085rm is o\u00a4ering a standard product and cannot observe consumer type, it will\nalso set the same price for all consumers. This makes the pre-AI game identical to a two-stage\nHotelling model, in which \u0085rms \u0085rst choose their product type (equivalent to their location)\nand then compete in prices. Throughout, I assume that\n5\f < 4\u000b; (1)\nwhich is su\u00a2 cient to ensure that the market is covered and the \u0085rms will not act as local monop-\nolies. As usual, I focus on subgame perfect equilibria, but with a slight abuse of terminology,\n11",
        "89c50b01-d0e9-4c7c-9fc5-f0350a117756": "I refer to these as \u0093equilibria\u0094 .\nIt is straightforward to see that the unique equilibrium in this model, as in the baseline\nHotelling model with quadratic transport costs, is maximal product di\u00a4erentiation (Tirole,\n1989). In this setting, this means that the two \u0085rms will o\u00a4er products at the two ends of the\nline (x0= 0andx1= 1) and set equilibrium prices given by p0=p1=\f, sharing the market\nequally. For future reference, I also note that in this equilibrium total \u0085rm pro\u0085ts are equal to\n\u0005pre-AI=\u00190+\u00191=\f, and consumer surplus is\nCSpre-AI=\u000b\u00002\fZ1=2\n0x2dx\u0000\f\n=\u000b\u000013\n12\f;\nwhere the \u0085rst line of this expression uses the symmetry between the \u0085rms and consumers on\nthe two sides of 1=2.\nAfter advances in AI, one of the \u0085rms, say \u0085rm 1, can use data from its previous customers\n(those with i\u00151=2) to predict their type and customize their products and prices.2In partic-\nular, I assume that in this post-AI environment, \u0085rm 1 can observe the type of any consumer\ni\u00151=2that visits its website and o\u00a4ers a customized bundle (x1\ni;p1\ni)to this consumer. For\nsimplicity, let us assume that \u0085rm 0 cannot do so and also that \u0085rm 1 cannot simultaneously\no\u00a4er customized and standardized products. Now in equilibrium, \u0085rm 1 will o\u00a4er each con-\nsumer with i\u00151=2a customized product x1\ni=i. It will also charge higher prices. The exact\nform of the equilibrium depends on \u0085rm 0\u0092 s product choice, which, given its inability to use\nthe new AI technology, cannot be customized. It is straightforward to see that \u0085rm 0 will also\nchange its product, because it no longer needs as much product di\u00a4erentiation (since \u0085rm 1\nwill be charging higher prices). The unique post-AI equilibrium is one in which \u0085rm 0 changes\nits standardized product to x0= 1=4. It then sets a price that makes the consumers that are\nfarthest away from it indi\u00a4erent between buying its product and not doing so, i.e.,3\np0=\u000b\u0000\f\n16:\nIt is also straightforward to see that it is optimal for \u0085rm 1 to set:\np1\ni=\u000bfor alli\u00151\n2;\n2More generally, the fact that AI-intensive \u0085rms are using data from and customizing products to their\nexisting customers introduces intertemporal linkages, which could create lock-in e\u00a4ects and rich-get-richer\ndynamics, as in the switching cost and dynamic oligopoly literatures, such as in Klemperer (1995) and Budd,\nHarris and Vickers (1993).\n3Firm 0 could o\u00a4er a lower price and steal some customers from \u0085rm 1, but it can be veri\u0085ed that this would\nlead to lower pro\u0085ts.\n12",
        "d2567aaf-b7bd-4a17-82bb-33913fa766ea": "thus capturing all the consumer surplus from the consumers about whom it has data.4In this\nequilibrium, we have \u0005post-AI=\u000b\u0000\f\n32>\u0005pre-AI(which is guaranteed by (1)), while consumer\nsurplus is now\nCSpost-AI=\u000b\n2\u00002\fZ1=4\n0x2dx\u00001\n2\u0012\n\u000b\u0000\f\n16\u0013\n=1\n48\f:\nAs a consequence, consumer surplus is much lower in this case. This can be seen most clearly\nby considering the limit where \f!0, in which case the pre-AI consumer surplus is maximal\n(approaching \u000b), while post-AI it becomes minimal (approaching 0). The negative impact of\nAI technologies on consumer surplus has two interrelated causes. First, \u0085rm 1 now uses its\nbetter prediction power to capture all the surplus from the consumers, even though it is in\nprinciple o\u00a4ering a better product and could have increased consumer welfare. Second, given\n\u0085rm 1\u0092 s more aggressive pricing, \u0085rm 0 is also able to capture more pro\u0085ts, reducing even the\nsurplus of consumers whose data are not being used.\nIt is worth noting that, in the present model there is no intensive margin of consumer choice\nand the market is covered (under (1)). As a result, AI does not a\u00a4ect quantity purchased, and\neven when it reduces consumer welfare, it increases utilitarian welfare \u0097 in particular, greater\ncustomization reduces \u0093transport costs\u0094 . The logic of the model highlights that this need not\nbe the case when there is a quantity/intensive margin, because higher markups may ine\u00a2 ciently\nreduce quantity purchased. We will see in the next subsection that there are other reasons for\nine\u00a2 ciency in similar environments.\nIn summary, the general lessons from this model are complementary to the ones from the\nprevious subsection:\n1. The use of AI technologies and detailed consumer data for prediction may improve the\nability of \u0085rms to customize products for consumers, potentially improving overall sur-\nplus.\n2. However, it also increases the power of (some) companies over consumers.\n3. This has direct distributional implications, enabling AI-intensive \u0085rms to capture more\nof the consumer surplus.\n4If we had allowed this \u0085rm to also market a standardized product, it would additionally compete for\nconsumers i <1=2, about whom it has no data. Our assumption rules out this possibility.\n13",
        "52026d31-b4ed-4b78-9486-5b42c800b785": "4. The indirect e\u00a4ect of the better collection and processing of data by one \u0085rm is to relax\nprice competition in the market, increasing prices and amplifying the direct distributional\ne\u00a4ects.\nAlthough in this model the overall surplus in the economy increases after the introduction\nof AI technologies, in the previous subsection we saw that this is not necessarily true in the\npresence of other data-related externalities, and in the next subsection we will encounter a new\neconomic force distorting the composition of products o\u00a4ered by platforms.\n2.3 Behavioral Manipulation\nThe previous subsection discussed how even the bene\u0085cial use of improved prediction about\nconsumer preferences and behavior might have a downside. But improved prediction tools\ncan also be put to nefarious uses, with potentially far-ranging negative e\u00a4ects. Platforms\nthat collect and e\u00a4ectively process huge amounts of data might able to predict consumer\nbehavior and biases beyond what the consumers themselves can know or understand. Anecdotal\nexamples of this concern abound. They include the chain store Target successfully forecasting\nwhether women are pregnant and sending them hidden ads for baby products, or various\ncompanies estimating \u0093prime vulnerability moments\u0094 and send ads for products that tend\nto be purchased impulsively during such moments. They also include marketing strategies\ntargeted at \u0093vulnerable populations\u0094such as the elderly or children. Less extreme advertising\nstrategies also have elements of the same type of manipulation, for example, when websites\nfavor products such as credit cards or subscription programs with delayed costs and short-\nterm bene\u0085ts or when YouTube and Facebook use their algorithms to estimate and favor more\naddictive videos or news feeds for the user group in question. As legal scholars Hanson and\nKysar have noted, \u0093Once one accepts that individuals systematically behave in non-rational\nways, it follows from an economic perspective that others will exploit those tendencies for\ngain.\u0094(1999, p. 630).\nThough these concerns are as old as advertising itself, economists and policy-makers hope\nthat consumers will learn how to shield themselves against abusive practices. The sudden\nexplosion in the capabilities of digital platforms to use AI technologies and massive data sets\nto improve their predictions undercuts this argument, however. Learning dynamics that had\nmade consumers well adapted to existing practices would be quickly outdated in the age of\nAI and big data. This issue is explored in Acemoglu et al. (2022), using a continuous-time\nlearning model. Here I outline a similar idea in a much simpler setting.\n14",
        "b2e20678-89f2-4376-9b2e-a39bdf5d44a7": "I consider a dynamic setting with two periods, t= 0;1, and no discounting. Consumers\nhave a choice between two products, x1andx2, in both periods. They are initially uncertain\nabout which one will yield higher utility. Suppose in particular that the true utility that a\nconsumer gets from the product is either HorL= 0. The prior belief of individual iis that\nthese two products will yield high utility for them is, respectively, q1\niandq2\ni.\nBoth products are produced and o\u00a4ered by a digital platform, which again has zero cost\nof production and can o\u00a4er personalized prices. To start with, the platform and the consumer\nhave symmetric information, and thus the platform knows and shares the consumer\u0092 s prior\nbeliefs. Once an individual consumes one of the two products, she obtains an additional piece\nof information about her utility from the product. I assume, in particular, that if the true\nquality isH, the consumer receives a positive signal, denoted by \u001bH(with probability 1).\nHowever, if the true quality is L, the product might still have deceptively high instantaneous\nutility (but long-term costs). Thus with probability \u0015, the consumer will receive the high\nsignal (and receives the low signal \u001bLwith complementary probability). The most relevant\ninterpretation of this \u0093false-positive\u0094 signal is that there are certain types of products that\n(predictably) appear more attractive to consumers, for example because of their tempting\nshort-term bene\u0085ts or because of their hidden negative attributes.\nLet us assume that the platform perfectly observes the consumer\u0092 s experience with the\nproduct she has consumed, and can change its pricing and product o\u00a4ering in the next period.\nThe game ends at the end of the second period.\nThe pre-AI equilibrium takes a simple form. The platform will o\u00a4er whichever product has\nhigherqifor consumer i, say product j, and will set the price\npj\ni;0=qj\niH,\ncapturing the full surplus. If the signal after consumption is \u001bL, then in the next period, it\nwill o\u00a4er the other product, ~j, charging the lower price\np~j\ni;1=q~j\niH,\nonce again capturing the full surplus. If, on the other hand, the signal is \u001bH, then in the second\nperiod, the same product will be o\u00a4ered, but now there will be a higher price. I assume that\nin the pre-AI environment, consumers have su\u00a2 cient experience with such products and the\nsignals they generate that they can correctly anticipate the likelihood of a high-quality product\ngiven a positive signal. As a result, the price following a positive signal will not increase all the\n15",
        "a944efab-88ea-48e9-8942-4fe2d04d8b72": "way toH. Rather, it will be given by the expected value of the product\u0092 s quality conditional\non a positive signal. A simple use of Bayesian updating gives this price as\npj\ni;0=qj\ni\nqj\ni+ (1\u0000qj\ni)\u0015H\n= \u0004j\niH,\nwhich again captures the full surplus from the consumer and also de\u0085nes the expression \u0004j\ni,\nwhich is convenient for the remainder of this section. (There is no option value term in prices,\nbecause the full surplus is being captured by the platform).\nThe deployment of AI technologies once again improves the platform\u0092 s ability to predict\nconsumer preferences and behavior \u0097 because it has access to the data from many similar\nconsumers and their experiences with similar products. As pointed out above, I assume that\nthis goes beyond what the consumer herself knows. In particular, I suppose that the platform\ncan now forecast whether the consumer will receive the high signal from a truly low-quality\nproduct. This, more generally, captures the ability of the platform to predict whether the\nindividual will engage in an impulse purchase or make other choices with apparent short-term\nbene\u0085ts and long-term costs.\nPost-AI, therefore, the relevant state for consumer iat timet= 0becomes\u0010\b\nqj\ni;\u0018j\ni\t\nj=1;2\u0011\n,\nwhere\u0018j\ni= 1designates the event that product jwill generate a false-positive signal \u0097 which\nmeans that in reality it is low-quality for the consumer, but still the signal \u001bHwill be realized\nif the consumer purchases it. Critically, the platform observes \u0018j\ni, but the consumer does not.\nFollowing Acemoglu et al. (2022), I assume that consumers are \u0093semi-behavioral\u0094 and do\nnot fully take into account that in the post-AI world, the platform actually knows \u0018j\ni. This\ncaptures the more general economic force mentioned above: in the pre-AI, business-as-usual\nworld, consumers may have learned from their repeated experiences and purchases, accurately\nestimating the relevant probabilities. The post-AI world is new and it is less plausible to expect\nthat the consumers will immediately understand the superior information that the platform has\nacquired. Note also that although it can forecast \u0018j\ni, the platform cannot observe consumer\npreferences perfectly, and when \u0018j\ni= 0, it does not know whether the product is high or\nlow-quality.\nWhat does equilibrium look like in the post-AI world? The key observation is that, while\nbefore AI the platform\u0092 s prediction was aligned with the prior of the household, this is no longer\nthe case in the post-AI world. In particular, suppose that we have q1\ni>q2\ni, but\u00181\ni= 0, while\n\u00182\ni= 1. Then the platform may prefer to o\u00a4er the second product. To understand this choice,\n16",
        "be373ab1-454d-47bb-81b7-21e7a9e9b808": "let us compute the pro\u0085ts from consumer iwhen the platform is using these two strategies.\nWhen it o\u00a4ers product 1, its total pro\u0085ts are\n\u00191\ni=\u0002\nq1\ni+q1\ni\u00041\ni+ (1\u0000q1\ni)q2\ni\u0003\nH:\nThis expression follows by noting that the platform is at \u0085rst o\u00a4ering product 1 and charging\nq1\ni. Because\u00181\ni= 0, the consumer will receive a positive signal only if the product is truly high\nquality, which happens with probability q1\ni. However, as indicated by the above discussion, in\nthis case, the consumer does not know whether this was a false-positive or a truly high-quality\nproduct, and thus her valuation will be \u00041\niH, which explains the second term. Finally, if she\nreceives a negative signal (probability 1\u0000q1\ni), in the second period, the platform will o\u00a4er\nproduct 2, charging q2\ni.\nOn the other hand, when it initially o\u00a4ers product 2, the platform\u0092 s pro\u0085ts are\n\u00192\ni=\u0000\nq2\ni+ \u00042\ni\u0001\nH;\nbecause in this case there will be a positive signal for sure.\nIt is straightforward to see that o\u00a4ering the second good is more pro\u0085table for the platform\nwhen\nq2\ni\nq2\ni+ (1\u0000q2\ni)\u0015>q1\ni(1\u0000q2\ni) +(q1\ni)2\nq1\ni+ (1\u0000q1\ni)\u0015: (2)\nCondition (2) is always satis\u0085ed whenever q2\niis su\u00a2 ciently close to q1\ni. Intuitively, the platform\nis willing to sacri\u0085ce a little bit of revenue in the \u0085rst period for the certainty of getting the\nconsumer to experience a good that it knows she will like \u0097 even though this is not a truly\nhigh-quality good.\nWhat about consumer welfare? Perhaps paradoxically, in the \u0085rst case, the consumer\nactually has a positive welfare. This is because in this case we have a high-quality product\n(and the platform indirectly recognizes this following the realization of signal \u001bH, because it\nknows that \u00181\ni= 0), and hence the positive signal can come only from a truly high-quality\ngood. It is then straightforward to compute the user\u0092 s welfare as\nU1\ni=q1\ni\u0012\n1\u0000q1\ni\nq1\ni+ (1\u0000q1\ni)\u0015\u0013\nH\n=q1\ni(1\u0000q1\ni)\u0015\nq1\ni+ (1\u0000q1\ni)\u0015H > 0:\nThis positive surplus may appear as the good side of our behavioral assumption. But the\nplatform\u0092 s second strategy shows the dark side. With this strategy, the consumer will overpay\n17",
        "ce66fc59-06a5-47a5-8a4a-e3392bf234ba": "in the second period (because, given \u00182\ni= 1, the product is in reality low-quality). Hence her\nutility is\nU2\ni=\u0000q2\ni\nq2\ni+ (1\u0000q2\ni)\u0015H < 0:\nTherefore, the ability of the platform to predict the consumer\u0092 s preferences and vulnerabilities\nleads to a situation in which the platform can increase its pro\u0085ts by marketing low-quality\nproducts that are likely to appeal to the consumer in the short run.\nIn contrast to the pattern in the previous subsection, this not only increases platform pro\u0085ts\nat the expense of consumers, but it also distorts consumption as it lures consumers towards\nlower-quality products, reducing utilitarian welfare.\nThe general lessons in this case are complementary but di\u00a4erent from the ones I highlighted\nin the previous two subsections:\n1. AI technologies can enable platforms to know more about consumers\u0092preferences than\nthey themselves do.\n2. This opens the way for potential behavioral manipulation, whereby the platform can o\u00a4er\nproducts that may temporarily appear as higher-quality than they truly are.\n3. This type of behavioral manipulation tends to do more than just shift surplus from\nconsumers to the platform; it also distorts the composition of consumption, creating new\nine\u00a2 ciencies.\n3 Labor Market E\u00a4ects of AI\nUS labor markets have not been doing well for workers over the last 40 years. Wage growth\nsince the late 1970s has been much slower than during the previous three decades, while the\nshare of capital in national income has grown signi\u0085cantly (Acemoglu and Autor, 2011; Autor,\n2019). Additionally, wage growth, such as it is, has been anything but shared. While wages\nfor workers at the very top of the income distribution \u0097 those in the highest tenth percentile\nof earnings or those with postgraduate degrees \u0097 have continued to grow, workers with a high\nschool diploma or less have seen their real earnings fall. Even college graduates have gone\nthrough lengthy periods of little real wage growth.\nMany factors have contributed to this sluggish average wage growth and real wage declines\nat the bottom of the distribution. The erosion of the real value of the minimum wage, which\n18",
        "9f54fb15-b635-474b-9893-16dfb4227a39": "has fallen by more than 30 percent since 1968, has been clearly important for low-wage workers\n(Lee, 1999). The decline in the power of trade unions and much of the private sector may have\nplayed a role as well. The enormous increase in trade with China also likely contributed, by\nforcing the closure of many businesses and large job losses in low-tech manufacturing industries\nsuch as textiles, apparel, furniture, and toys (Autor, Dorn and Hanson, 2013).\nMy own work with Pascual Restrepo (2019, 2021) emphasizes and documents the impor-\ntance of the direction of technological progress in this process. While in the four decades after\nWorld War II automation and new tasks contributing to labor demand went hand-in-hand, a\nvery di\u00a4erent path of technological development emerged starting in the 1980s, exhibiting more\nautomation and much slower advances in human-friendly technologies, such as those involving\nnew tasks (Acemoglu and Restrepo, 2019). Automation eliminated routine tasks in clerical\noccupations and on factory \u0087 oors, depressing the demand and wages of workers specializing\nin blue-collar jobs and clerical functions. Meanwhile professionals in managerial, engineering,\n\u0085nance, consulting, and design occupations \u0087 ourished \u0097 both because they were essential to\nthe success of new technologies and because they bene\u0085ted from the automation of tasks that\ncomplemented their own work. As automation gathered pace, wage gaps between the top\nand the bottom of the income distribution magni\u0085ed. In Acemoglu and Restrepo (2021), we\nestimate that automation has been possibly the most important factor in reshaping the US\nwage structure, explaining somewhere between 50 to 70% of the variance of changes in wages\nby demographic group between 1980 and 2016.\nAll of this predates AI. In Acemoglu et al. (2021), we \u0085nd that AI activity across US\nestablishments picks up speed only after 2016. Nevertheless, this background is useful because\nAI may be the next phase of automation, and there is evidence that it is already being used both\nfor automation and for tighter monitoring of workers, further depressing wages and the labor\nshare. In this section, I \u0085rst explain how automation works and why we may be concerned\nabout excessive automation in general, and how AI may exacerbate these concerns. I then\ndiscuss how AI could be used for generating new tasks and technologies that complement\nhumans, but whether this will be the case or not depends on technology adoption and research\nand development choices of companies. In this context, I suggest reasons for being concerned\nthat the composition of AI research may be heavily distorted. I also discuss why the most\nbenign view of AI\u0092 s role in the labor market \u0097 automating routine jobs, so that workers have\ntime for more creative, problem-solving tasks \u0097 may need to be quali\u0085ed. Finally, I explore\nhow AI may have pernicious e\u00a4ects when it is used for monitoring.\n19",
        "e1cfa30b-e39c-4366-a030-4178176d239e": "3.1 Excessive Automation and AI\nIn order to situate the role of AI in the broader context of automation technologies, I start\nwith a review of the framework from Acemoglu and Restrepo (2018, 2019), which models the\nautomation of tasks (as well as the creation of new tasks). Suppose there is a single good in\nthe economy, Y, whose production requires the combination of a measure 1 of tasks:\nY=\u0012ZN\nN\u00001Y(z)\u001b\u00001\n\u001bdz\u0013\u001b\n\u001b\u00001\n; (3)\nwhereY(z)denotes the output of task zand\u001b\u00150is the elasticity of substitution between\ntasks. The key economic decision is the allocation of tasks to factors. Let me focus on just two\nfactors, capital and labor, and suppose as in Acemoglu and Restrepo (2018, 2019) that each\nfactor has task-speci\u0085c productivities, determining its comparative advantage, and only tasks\nz\u0014Ican be automated given the current level of automation technology. This implies:\nY(z) =\u001aAL\rL(z)l(z) +AK\rK(z)k(z)ifz2[N\u00001;I]\nAL\rL(z)l(z) ifz2(I;N ]:\nHerel(z)andk(z)denote the total labor and capital allocated to producing task z. The state of\ntechnology is captured by the following: factor-augmenting terms, ALandAK, which increase\nthe productivity of the relevant factor uniformly in all tasks; task-speci\u0085c productivities, \rL(z)\nand\rK(z), which increase the productivity of a factor in a speci\u0085c task; the threshold for\ntasks that are feasible to automate, I; and the measure of new tasks, N. Let us assume that\n\rL(z)=\rK(z)is increasing in z, so that labor has a comparative advantage in higher-indexed\ntasks. Suppose that capital is produced from the \u0085nal good, with marginal cost R, which also\ngives its rental rate. Labor is inelastically supplied, with total supply given by L, and the\nequilibrium wage is denoted by w.\nAcemoglu and Restrepo (2018, 2019) characterize the competitive equilibrium in this econ-\nomy. Here I allow both competitive and rigid labor markets, by assuming that the wage cannot\nfall below some level w. In this case, the equilibrium wage can be written as:\nw= maxfw;MPL (L)g;\nwhere MPL (L)is the marginal product of labor when there is full employment at L. The wage\n\u0087 oor may be a consequence of regulations, such as minimum wages and union-imposed minima,\nor may result from other labor market imperfections, such as e\u00a2 ciency wage considerations.\n20",
        "68cce1e2-ebd3-4f80-9f3a-2cf7c3e456fa": "Let us \u0085rst focus on how the marginal product of labor changes (without any wage \u0087 oor).\nFollowing Acemoglu and Restrepo (2018), this is given by\n@lnMPL (L)\n@I=@lnY(L;K )\n@I(Productivity e\u00a4ect) (4)\n+1\n\u001b1\u0000sL\n1\u0000\u0000(N;I )@ln \u0000(N;I)\n@I(Displacement e\u00a4ect)\nwheresLdenotes the labor share and \u0000(N;I ) =RN\nI\rL(z)\u001b\u00001dzRI\nN\u00001\rK(z)\u001b\u00001dz+RN\nI\rL(z)\u001b\u00001dzis a measure of the\nlabor\u0092 s task content of production (capturing what fraction of tasks are assigned to labor). In\nthe special cases where \u001b= 1or where\rK(z) =\rL(z), we have \u0000(N;I ) =N\u0000I, but more\ngenerally, \u0000(N;I )is always increasing in Nand decreasing in I. The \u0085rst line of (4) represents\ntheproductivity e\u00a4ect, which is driven by the fact that automation reduces costs and thus\nincreases productivity \u0097 by an amount equivalent to the cost di\u00a4erence between producing\nthe marginal tasks by labor vs. capital:\n@lnY(L;K )\n@I=1\n\u001b\u00001\"\u0012R\nAK\rK(I)\u00131\u0000\u001b\n\u0000\u0012MPL (L)\nAL\rL(I)\u00131\u0000\u001b#\n:\nThe second line is the displacement e\u00a4ect created by automation: as tasks are allocated away\nfrom labor towards capital, the marginal product of labor declines. This displacement e\u00a4ect,\nwhich reduces the range of tasks employing workers, is always negative.\nWhen we are at full employment, (4) gives the impact of automation on wages. When,\ninstead, the wage \u0087 oor at wis binding, then the same e\u00a4ects now impact employment. The\nonly di\u00a4erences are that on the left-hand side of (4), we now have the proportional change in\nemployment, and on the right-hand side, MPL (L)=AL\rL(I)is replaced by w=AL\rL(I).\nLet us \u0085rst consider full employment. What happens to the labor market equilibrium\nfollowing additional automation? Equation (4) \u0085rst shows that the labor share will always\ndecline \u0097 because of the displacement e\u00a4ect, the wage will increase less than proportionately\nwith productivity. Equally importantly, the wage level may fall as well. This is because\nthe displacement e\u00a4ect can be larger than the productivity e\u00a4ect. In particular, when the\nproductivity e\u00a4ect is small, for example, in the edge case where w=AL\rL(I)\u0019R=AK\rK(I),\nthere is no productivity e\u00a4ect, and the equilibrium wage will necessarily decline. When there\nis more than one type of labor, the same argument also implies that the average wage may\nfall, though the wage of some groups may increase (see Acemoglu and Restrepo, 2021).\nThis framework further clari\u0085es why automation could reduce employment. Suppose the\nwage \u0087 oorwis binding and again take the edge case where w=AL\rL(I)\u0019R=AK\rK(I), so that\n21",
        "3bb34eb1-cd26-4f62-8e00-9828794b5192": "the productivity e\u00a4ect is approximately zero. Then, automation necessarily reduces employ-\nment. By continuity, the same happens when the productivity e\u00a4ect is positive but not too\nlarge \u0097 the case that Acemoglu and Restrepo (2019) refer to as \u0093so-so technologies\u0094 , because\nthey are good enough to be adopted, but not so good as to have a meaningful impact on\nproductivity.\nWhat are the welfare consequences of employment-reducing automation? In a perfectly\ncompetitive market, where workers are at the margin indi\u00a4erent between leisure and work, and\nwhen there are no other distributional concerns, an automation-induced decline in employment\ndoes not have \u0085rst-order welfare consequences. In fact, it is straightforward to see that the\ncompetitive equilibrium would always maximize net output (de\u0085ned as total production minus\nwhat is used up for producing capital). However, when there are labor market imperfections,\nsuch as those captured by the wage \u0087 oor w, then low-productivity automation reduces welfare\n\u0097 thus motivating the term \u0093excessive automation\u0094 . This can be seen with the following\nargument: because the productivity e\u00a4ect is approximately zero, gross output and pro\u0085ts do\nnot increase (workers in marginal tasks are replaced by machines, but total costs have not\nchanged). Yet, capital usage increases, and this reduces net output. At this point, reallocating\nmarginal tasks away from capital towards labor \u0097 thus reducing automation \u0097 would increase\nnet output.\nWhy is the equilibrium misaligned with social welfare maximization? The answer is related\nto the wage \u0087 oor. Firms, when making their hiring and automation decisions, are responding\nto the market wage, w, whereas a utilitarian social planner \u0097 seeking to maximize net surplus\n\u0097 should take into account the opportunity cost of labor, which is zero. This argument\nestablishes that when productivity e\u00a4ects are limited, there will be excessive automation. It\nalso pinpoints one of the channels for this type of ine\u00a2 ciency: in economies with labor market\nimperfections, \u0085rms base their automation decisions on the higher wage rate, rather than the\nlower social opportunity cost of labor.\nThis argument also clari\u0085es that automation is likely to be excessive and potentially welfare-\nreducing especially when it generates small or negligible productivity e\u00a4ects. If the productivity\ngains from automation had been large, net output would have increased, even if it displaced\nworkers. Moreover, equation (4) highlights that with a large productivity e\u00a4ect, there may not\nhave been a decline in labor demand in the \u0085rst place.\nThe case for excessive automation is strengthened if there are other considerations favoring\nhigher levels of employment. For example, if employed individuals generate positive external\n22",
        "1db42bb3-284a-49c4-aacf-6e286f84eefa": "e\u00a4ects (on their families and communities or for democracy) relative to the unemployed, then\nthe social planner may want to increase employment beyond the equilibrium level. Distrib-\nutional concerns would also weigh in the same direction, since, in general, automation helps\n\u0085rms and \u0085rm owners, while reducing the labor share. In addition, as shown in Autor, Levy\nand Murnane (2003) and Acemoglu and Restrepo (2021), automation boosts inequality across\nworker groups, creating another distributional cost.\nWhat does this imply for AI? AI is a broad technological platform, and can be used for\ndeveloping many di\u00a4erent types of technologies. Automation, especially automation of various\nwhite-collar tasks and jobs with a signi\u0085cant decision-making component, is one of these appli-\ncations. If AI is used for automation, then the arguments outlined above would also imply that\nlow-productivity AI may reduce welfare. Two key questions are thus whether AI technologies\nare likely to be deployed for substituting capital and algorithms for labor in various tasks and\nwhether this will generate small or large productivity gains. The evidence in Acemoglu et\nal. (2021) suggests that there has been a signi\u0085cant uptick in AI activities since 2016, and\nmuch of this has been associated with task displacement. That paper also \u0085nds reduced hiring\nin establishments adopting AI technologies, so the evidence is consistent with, though does\nnot prove, the idea that new AI technologies may not be improving productivity su\u00a2 ciently.\nThere are other reasons why productivity gains from AI may be small. Most importantly, AI\ntechnologies are being used in some tasks in which humans are quite good (natural language\nprocessing, facial recognition, problem-solving; see Acemoglu, 2021).\nIn summary, the general lessons from this section are:\n1. Automation reduces the labor share and may also reduce the (average) wage and/or em-\nployment, and this latter outcome is more likely when productivity gains from automation\nare small.\n2. When labor market imperfections create a wedge between the market wage and the\nsocial opportunity cost of labor, automation tends to be excessive and welfare-reducing,\nparticularly when it impacts employment negatively as well. This too is more likely\nto be the case when its productivity e\u00a4ects are small. The same considerations apply\nwhen there are non-market reasons for preferring high levels of employment (for example,\nbecause employed workers contribute more to their families, communities or society in\ngeneral).\n3. Because it increases the capital share and reduces the labor share and because it boosts\n23",
        "d031f2d3-e8c5-43eb-9c1d-d04f78a1089f": "inequality among workers, automation may also be excessive from a welfare point of view\ndue to distributional concerns\n4. If AI is used predominantly for automation, it will have similar e\u00a4ects to other automation\ntechnologies, and depending on its productivity e\u00a4ects and relevant welfare criteria, it\nmay have a negative impact on social welfare.\n3.2 Direction of AI Technology and its Labor Market Consequences\nThe previous subsection discussed some implications of AI used for automation, but it also\nnoted that AI, as a broad technological platform, can be used for creating new tasks or in-\ncreasing labor productivity as well. In the framework of the previous subsection, this would\ncorrespond to an increase in N. The framework presented in the previous section additionally\nimplies that new tasks increase the labor share and raise wages or employment (or both). In\nparticular, as in Acemoglu and Restrepo (2018, 2019), we now have\n@lnMPL (L)\n@N=@lnY(L;K )\n@N(Productivity e\u00a4ect) (5)\n+1\n\u001b1\u0000sL\n1\u0000\u0000(N;I )@ln \u0000(N;I)\n@N: (Reinstatement e\u00a4ect)\nThe productivity e\u00a4ect is positive as usual (even if the exact sources of productivity gains from\nnew tasks are di\u00a4erent than those from automation). In addition, the reinstatement e\u00a4ect is\nalso positive, because it is driven by the fact that new labor-intensive tasks are reinstating\nlabor back into the production process. As a result, new tasks always increase employment\nand/or wages. Moreover, the presence of the reinstatement e\u00a4ect implies that the wage bill\nincreases proportionately more than the productivity gains, pushing up the labor share \u0097 the\nconverse of the impact of automation. Acemoglu and Restrepo (2019) have argued that the\nreason why wages grew robustly during the decades following World War II was that rapid\nautomation in certain tasks went hand-in-hand with the introduction of su\u00a2 ciently many new\ntasks, counterbalancing the labor market implications of automation.\nReturning to the implications of AI, with the same argument, using AI for new tasks\nwould be welfare-improving, especially when there are labor market imperfections or other\nconsiderations favoring higher levels of employment than in equilibrium. Furthermore, if AI\nboosts the creation of new tasks and improves human productivity, it could counterbalance\nsome of the adverse e\u00a4ects of automation based on other technologies (such as robotics or\nspecialized software).\n24",
        "ca58f624-ec0a-49a7-990e-cf9df16fbfce": "When AI can be used both for automation and for new tasks, the pivotal question becomes\nhow the balance between these two activities is determined \u0097 that is, the direction of tech-\nnological change. Acemoglu and Restrepo (2018) provide a framework for the analysis of the\nequilibrium direction of technology. Their framework emphasizes the role of factor prices and\nthe labor share and highlights that there are reasons for optimal and equilibrium allocations\nto di\u00a4er. In particular, labor market imperfections not only promote too much automation \u0097\nas we saw in the previous subsection \u0097 but could further lead to an unbalanced composition\nof AI research between automation and new tasks.\nThere may also be potential distortions in the direction of technological change that go\nbeyond the purely economic. In Acemoglu (2021) I emphasize that the direction of technology\nis partly shaped by the business models of leading \u0085rms and the aspirations of researchers,\nand if these favor automation, the equilibrium may involve too much automation, even absent\neconomic distortions. Another related argument is that US corporations may have become\ntoo focused on cost-cutting, which might also encourage excessive automation. Acemoglu,\nManera and Restrepo (2020), on the other hand, show that the US tax code imposes a much\nhigher marginal tax rate on labor than equipment and software capital, thus favoring automa-\ntion. This policy channel triggers both excessive adoption of automation technologies and\ndisproportionate emphasis on automation in research and development.\nAI as a technological platform could in principle boost e\u00a4orts to create new tasks. Take\neducation as an example. Current investments in this area are focused on using AI technologies\nfor automated grading and the development of online learning tools to replace various tasks\nperformed by teachers. Yet, AI can be deployed for creating new tasks and directly increasing\nteacher productivity as well. It can be used for adapting teaching material to the needs and\nattitudes of diverse students in real time, overcoming a major problem of classroom-based\nteaching \u0097 the fact that students have diverse strengths and weaknesses and \u0085nd di\u00a4erent\nparts of the curricula more challenging (see the discussion in Acemoglu, 2021). Likewise, AI has\nmany diverse applications in health that can personalize care and empower nurses and general\npractitioners to make more and better decisions in care delivery. These potentially promising\ndirections notwithstanding, AI may be more likely to aggravate excessive automation. The\ncurrent trajectory in AI research is shaped by the visions of large tech companies, who are\nresponsible for the majority of the spending on this technology. Many of these companies\nhave business models centered on substituting algorithms for humans, which may make them\nfocus excessively on using AI for automation. At the same time, many AI researchers focus\n25",
        "c5a4d653-7d3d-4245-910a-475d0ca2a488": "on reaching \u0093human parity\u0094in narrow tasks as the main metric of success, which could create\nanother powerful force towards automation, rather than using this platform for creating new\ntasks. Like other automation technologies, AI may also appeal to many executives intent on\ncost-cutting, and if there are additional tax breaks and favorable treatments for software in\ngeneral and AI-related technologies speci\u0085cally, these may exacerbate the focus on automation\n(Acemoglu and Johnson, 2022).\nOverall, even though there is no de\u0085nitive evidence on this question, it is possible that the\ndirection of technological change was already tilted too much towards automation before AI,\nand AI may have exacerbated these trends. If so, its labor market implications could be one\nof the major harmful e\u00a4ects of AI.\nThe general lessons from this discussion are therefore:\n1. AI could in principle be used for increasing worker productivity and expanding the set\nof tasks in which humans have a comparative advantage, rather than focusing mainly on\nautomation. If it is used in this way, it may counterbalance some of the negative e\u00a4ects\nof automation on labor and may generate more positive welfare e\u00a4ects and bene\u0085cial\ndistributional consequences\n2. But there is no guarantee that the composition of technological change in general and the\nbalance of AI between automation and more human-friendly activities should be optimal.\nIn fact, there are many possible distortions, some of them economic and some of them\nsocial, encouraging excessive automation using AI.\n3.3 AI and Human Judgment\nThe arguments in the previous two subsections are partly predicated on the notion that AI-\nbased automation may not generate sweeping productivity gains, which could compensate for\nor even undo the displacement e\u00a4ects it creates. AI\u0092 s most enthusiastic boosters, on the other\nhand, believe that AI can bring huge productivity gains. One of the most powerful arguments in\nthis respect is that as AI helps automate and improve (both cognitive and noncognitive) tasks\nthat do not require human judgment and creativity, it will increase the demand for problem-\nsolving tasks that require creativity and judgment and also free workers to focus on these tasks.\nAlthough seemingly plausible, I now suggest a potential reason why this expectation may be\ntoo optimistic and argue that, even when such reallocation takes place, AI-based automation\nmay be excessive.\n26",
        "4a8ebaf4-91ad-42c8-a06b-fbaf87579606": "Suppose that there are two tasks to be performed, 1and2. Overall output in the economy\nis given by\nY= min\b\ny1;y2\t\n;\nwhereyiis the output of task i, and the Leontief production function imposes that these tasks\nare strongly complementary.\nBefore AI, both tasks have to be performed by humans. Suppose that there is a measure 1\nof humans, each with 2 units of time. Suppose also that, for reasons I will explain below, we\nstart in an allocation in which each human allocates half of their time to task 1 and the other\nhalf to task 2. In this case, they have equal productivity in both tasks, which I normalized to 1.\nAs a result, before AI, the economy produces a total of one unit of the \u0085nal good. We can think\nof each worker as a \u0093yeoman-producer\u0094 , consuming his or her production. Equivalently, we\ncan think of this economy as consisting of \u0085rms hiring workers in a competitive labor market.\nIn this case, the per hour wage of each worker in each task will be 1=2, ensuring that the entire\noutput is paid to workers.\nNow imagine that there are advances in AI algorithms that produce the \u0085rst task at per\nunit costc <1=2. This cost is paid in terms of the \u0085nal good, and the fact that it is less\nthan the equilibrium wage before AI implies that these algorithms are cost-saving and will\nbe adopted. If this were the end of the story, AI would improve net output, because workers\nwould be reallocated from task 1 to task 2, enabling the economy to increase its total output.\nHowever, suppose that there are also economies of scope: individuals learn from performing\nboth tasks at the same time (and that is why the pre-AI allocation involved each worker\ndevoting half of their time to each task). Suppose, in particular, that if a worker does not\nlearn from task 1, his or her productivity in task 2 declines to 1\u0000\f. The post-AI allocation\nwill involve all workers working in task 2, and whatever their total production is in this task,\nthe economy will also produce exactly the same amount of task 1, using AI algorithms. As a\nresult, net output in this economy will be\n2(1\u0000\f)\u0000spending on AI = 2(1\u0000\f)(1\u0000c):\nIt can be veri\u0085ed that in the special case where there are no economies of scope ( \f= 0),c<1=2\nis su\u00a2 cient for net output to increase \u0097 in particular, from 1to2(1\u0000c)>1. However, as\nsoon as\f >0, this is no longer guaranteed. For example, when c\u00191=2, even a small amount\nof economies of scope implies that the use of AI would reduce net output.\n27",
        "caec6e46-6891-4f79-8f62-73b52b9010cb": "This simple example captures a more general phenomenon: a \u0085ner division of labor and\nthe reallocation of some tasks away from humans can be cost-reducing, but to the extent\nthat human judgment improves when workers gain experience from dealing with a range of\nproblems and recognize di\u00a4erent aspects of the problem, it may also come at a cost. When\nsome aspects of the problem are delegated to AI, humans may start losing their \u0087 uency with\nand ability to understand the holistic aspects of relevant tasks, which can then reduce their\nproductivity, even in tasks in which they specialize. An extreme example of this phenomenon\ncan be given from the learning of mathematical reasoning. Calculators are much better than\nhumans in arithmetic. But if students stopped learning arithmetic altogether, delegating all\nsuch functions to calculators and software, their ability to engage in other type of mathematical\nand abstract reasoning may su\u00a4er. For this reason, most mathematical curricula still emphasize\nthe learning of arithmetic. If delegating certain tasks to AI becomes similar to students ceasing\nto learn arithmetic, it may have signi\u0085cant costs.\nWould the market adopt AI when there are such economies of scope? The answer depends\non the exact market structure. Because the adoption of AI technologies is associated with a\n\u0085ner division of labor, there is no guarantee that \u0085rms will internalize the economies of scope.\nFor example, in the pre-AI equilibrium, the cost of one unit of task 1 is 1/2. So a new \u0085rm\ncan enter with the AI technology and make pro\u0085ts in this equilibrium. The entry of these\n\u0085rms would then create a pecuniary externality, discouraging other workers from working on\ntask 1. In particular, even though there are economies-of-scope bene\u0085ts from performing task\n1, workers may not be allocated to task 1, because the price of this task is now lower due\nto the use of AI technologies.5This type of entry could then destroy the pre-AI equilibrium\nand would drive the economy to the post-AI equilibrium characterized above, even when it is\nine\u00a2 cient because \fis large.\nIn summary, the general lessons from this short discussion are:\n1. In addition to the costs of worker displacement discussed earlier in this section, economies\nof scope across tasks may create additional costs from the use of AI technologies. In par-\nticular, the deployment of AI in various cognitive tasks that do not require a high degree\n5There are some market structures and pricing schemes that may prevent the adoption of AI technologies\nwhen they are ine\u00a2 cient in this case. For example, if workers can take a very low or even negative wage in\norder to work in task 1 (so as to increase their productivity in task 2), this may outweigh the cost advantage\nof new \u0085rms that enter and specialize in using AI in task 1. The issues are similar to the ones that arise in the\ncontext of \u0085rm-sponsored general training, and as in that case, labor and credit market imperfections would\ntypically preclude the possibility that workers fully pay for all the bene\u0085ts they receive by taking wage cuts\n(see Acemoglu and Pischke, 1999).\n28",
        "725a1ce3-a8c8-47f3-9938-938e5c462d96": "of human judgment and creativity may enable workers to reallocate their time towards\ntasks that involve judgment and creativity. But if economies of scope are important for\nhuman productivity, AI may have additional costs.\n2. Cost-minimization incentives of \u0085rms may encourage them to use AI technologies in\nine\u00a2 cient ways, when there are such economies of scope.\n3.4 AI and Excessive Monitoring\nAnother use of AI-powered technologies is in worker monitoring, as exempli\u0085ed by Amazon\u0092 s\nwarehouses and new monitoring systems for delivery workers. Here, too, employers\u0092incentives\nto improve monitoring and collect information about their employees predates AI. But once\nagain, AI may magnify their ability to do so. Some amount of monitoring by employers may\nbe useful by improving worker incentives. However, I argue that increasing employer \u0087 exibility\nin this activity can also lead to ine\u00a2 ciently high levels of monitoring for a very simple reason:\nat the margin, monitoring is a way of shifting rents away from workers towards employers, and\nthus is not socially valuable.\nI now develop this point using a model based on Acemoglu and Newman (2002). Consider\na one-period economy consisting of a continuum of measure Nof workers and a continuum\nof measure 1of \u0085rm owners, each with a production function AF(Li)whereLidenotes the\nnumber of workers employed by \u0085rm iwho exert e\u00a4ort (the alternative, exerting zero e\u00a4ort,\nleads to zero productivity). Firms are large, so the output of an individual worker is not\nobservable. Instead, employers can directly monitor e\u00a4ort in order to determine whether an\nemployee is exerting e\u00a4ort and being productive.\nSpeci\u0085cally, as in Shapiro and Stiglitz (1984) a worker exerting e\u00a4ort is never mistakenly\nidenti\u0085ed as a shirker, and a shirking worker is caught with probability qi=q(mi)wheremiis\nthe extent of monitoring per worker by \u0085rm i, with cost CmiLi, whereC > 0. Suppose that\nqis increasing, concave and di\u00a4erentiable with q(0) = 0 andq(m)<1for allm. Suppose also\nthat because of the limited liability constraints, workers cannot be paid a negative wage and\nwill simply receive a zero wage. This implies a simple incentive compatibility constraint for\nworkers,\nwi\u0000e\u0015(1\u0000qi)wi;\nwhereedenotes the cost of e\u00a4ort. Rearranging this equation we obtain:\n29",
        "4ed8ded3-fe9b-4b26-a90d-5231fa0cf5bb": "wi\u0015e\nq(mi): (6)\nIn addition, the \u0085rm has to respect the participation constraint,\nwi\u0000e\u0015u; (7)\nwhereuis the worker\u0092 s ex ante reservation utility, given by what he could receive from another\n\u0085rm in this market.\nThe \u0085rm maximizes its pro\u0085ts, given by max\nwi;Li;qi\u0005 =AF(Li)\u0000wiLi\u0000CmiLisubject to these\ntwo constraints. As shown in Acemoglu and Newman (2002), the solution to this problem\ntakes a simple form because the incentive compatibility constraint always binds \u0097 if it did\nnot, the \u0085rm would reduce monitoring, increasing its pro\u0085ts. By contrast, the participation\nconstraint (7) may or may not bind.\nThe main result from this framework relevant for my discussion is that, regardless of whether\nthe participation constraint is binding, the equilibrium never maximizes utilitarian social wel-\nfare (total surplus), given by Y=AF(L)\u0000CmL\u0000eL, because there is always too much\nmonitoring. The intuition is straightforward: at the margin, monitoring is used to transfer\nrents from workers to \u0085rms, and is thus being used excessively. Mathematically, this can be\nseen in the following way: start from the equilibrium and consider a small decline in monitoring\ncoupled with a small increase in the wage so that (6) remains binding. This will have only a\nsecond-order impact on the \u0085rm\u0092 s pro\u0085ts, since the \u0085rm was already maximizing them. But it\nwill have a \u0085rst-order bene\u0085t for workers, whose wage will increase. Thus, a utilitarian social\nplanner would like to increase wages and reduce monitoring. This is true a fortiori if we care\nabout income inequality (presuming, of course, that \u0085rm owners are richer than workers).\nHow does AI a\u00a4ect things in this equilibrium? Suppose that before AI, there was an upper\nbound on monitoring, so that m\u0014\u0016m, and AI lifts this constraint. If the equilibrium level\nof monitoring without this constraint, m\u0003, is above \u0016m, then improvements in AI will lead to\nhigher monitoring. If, in addition, \u0016mis not too low, then AI would reduce welfare (the quali\u0085er\nthat \u0016mshould not be too low is included because, if it were very low, the initial equilibrium\ncould be very ine\u00a2 cient).\nThe economic force here is much more general than the simple model I presented: AI, by\nenabling better control and use of information, provides one more tool to employers to shift\nrents away from workers to themselves, and this generally leads to ine\u00a2 ciently high levels of\nrent-shifting activities.\n30",
        "565a37af-9233-4368-8498-e91d6a536157": "Is this potential ine\u00a2 ciency relevant? Once again, there is little systematic evidence to\nsuggest one way or another, but the fact that the US labor market has a \u0093good job\u0094problem,\nand wages at the bottom of the distribution have fallen in real terms over the last several\ndecades (Acemoglu, 2019; Acemoglu and Restrepo, 2021) suggests that it may be.\nIn summary, the general lessons from this analysis are:\n1. AI technologies also create new opportunities for improved monitoring of workers. These\ntechnologies have \u0085rst-order distributional consequences, because they enable better mon-\nitoring and thus lower e\u00a2 ciency wages for workers.\n2. Because at the margin the use of monitoring technologies transfers rents from workers to\n\u0085rms, monitoring will be excessive in equilibrium. By expanding monitoring opportuni-\nties, AI may thus create an additional social cost.\n4 AI, Political Discourse and Democracy\nThe 1990s witnessed rapid strengthening of democracy around the world, in a pattern the\npolitical scientist Samuel Huntington (1991) called \u0093The Third Wave\u0094 . During this process,\nmany Latin American, Asian and African countries moved from nondemocratic regimes towards\ndemocracy and several others strengthened their democratic institutions (Marko\u00a4, 1996). The\nlast decade and a half has witnessed a pronounced reversal of this process, however. Several\ncountries moved away from democracy, and perhaps even more surprisingly, Democratic insti-\ntutions and norms have come under attack in numerous Western nations (Levitsky and Ziblatt,\n2017; Snyder, 2017; Mishra, 2017; Applebaum, 2020) and the population appears to be more\npolarized than in the recent past (Abramowitz, 2010; Judis, 2016). Some have pointed to social\nmedia and online communication as major Contributing factors (e.g., Marantz, 2020). I now\nturn to a discussion of these issues. I focus on the e\u00a4ects of AI on communication, political\nparticipation and democratic politics. As indicated in the Introduction, I will highlight several\ndistinct but related mechanisms via which AI might degrade democratic discourse.\n4.1 Echo Chambers and Polarization\nSocial media is often argued to promote echo chambers, in which individuals communicate\nwith others who are like-minded, and this might prevent them from being exposed to counter-\nattitudinal viewpoints, exacerbating their biases. Cass Sunstein noted the potential dangers\n31",
        "e028c1c8-868a-4881-9dcd-45a7b6299a51": "of echo chambers as early as 2001. He stated that encountering individuals with opposing\nopinions and arguments is \u0093important partly to ensure against fragmentation and extremism,\nwhich are predictable outcomes of any situation which like-minded people speak only with\nthemselves\u0094 , and emphasized that \u0093many or most citizens should have a range of common\nexperiences. Without shared experiences, a heterogeneous society will have a much more\ndi\u00a2 cult time in addressing social problems.\u0094(Sunstein, 2001, p. 9). The recent documentary\nThe Social Dilemma describes the situation as \u0093The way to think about it is as 2.5 billion\nTruman Shows. Each person has their own reality with their own facts. Over time you have\nthe false sense that everyone agrees with you because everyone in your news feed sounds just\nlike you.\u0094AI is crucial to this new reality on social media. For example, the algorithms that\nsites such as Facebook and Twitter use in deciding what types of news and messages individuals\nwill be exposed to is based on applying AI techniques to the massive amount of data that these\nplatforms collect (Alcott and Gentzkow, 2017; Guriev, Henry and Zhuravskaya, 2020; Mosleh\net al., 2021). Recent studies document that these algorithmic approaches are exacerbating\nthe problem of misinformation on social media, for example by creating algorithmic \u0093\u0085lter\nbubbles\u0094 , whereby individuals are exposed much more frequently to news that agrees with\ntheir priors and biases (Levy, 2021). As a result, Vosoughi et al. (2018) conclude that on\nsocial media there is a pattern of \u0093falsehoods di\u00a4using signi\u0085cantly farther, faster, deeper, and\nmore broadly than the truth in all categories of information\u0094 .\nIn this subsection, I discuss these issues building on the approach in Acemoglu, Ozdaglar\nand Siderius (2021), which suggests that social media platforms may endogenously create such\necho chambers, and do so more when there is more extremist content around. Consider an\nonline community in which each individual receives a news item that is circulating (either from\nan outside source or from other members of the community). The news item may contain\nmisinformation (which could be downright fake news or some misrepresentation of facts). The\nindividual decides whether to share the news item she receives.\nEach agent receives utility from sharing news items. However, if she is found out to have\nshared misinformation, she incurs a cost. To avoid this, she may instead decide to inspect\nthe news item to check for misinformation and can then decide to kill it. Inspecting is costly.\nThus, all else equal, the individual is more likely to inspect and less likely to share a news item\nif she thinks it contains misinformation.\nSuppose that there are two sub-communities, one is left-wing and the other is right-wing,\nmeaning that one community consists of individuals with priors to the left of some reference\n32",
        "afb75264-e191-4f94-9bc4-e8df4d73ff85": "point, normalized 0, and the other one consisting of individuals with priors to the right of 0.\nFor simplicity, I suppose that the prior of each left-wing community member is the same, some\nbL<0, and the prior of each right-wing community member is also the same, bR>0. Each\nmember of one of these communities is much more likely to be connected to and sharing items\nwith other members of her community, but there are also some cross-community links. The\nextent of these cross-community links determines how much \u0093homophily\u0094there is. With high\nhomophily, there are almost no cross-community links. With low homophily, cross-community\nlinks are more common.\nEach news item has a type consisting of: (1) a news-related message, which is simply taken\nto bem2fL;Rg, and (2) a reliability score r\u00150, whereby a high reliability score means\nthat the news item is very unlikely to contain misinformation, while a low reliability score\nmeans misinformation is quite likely. Although this setup is a simpli\u0085ed version of the one in\nAcemoglu, Ozdaglar and Siderius (2021), a full analysis of the model is still involved. Here I\nsummarize the main features of the equilibrium.\nSuppose that individuals do not observe the provenance, source and history of the message\nand do not know whether the message was inspected by others in the past.\nConsider a right-wing agent receiving a left-wing message. Given her prior, she is much\nmore likely to think that this message contains misinformation than a right-wing message.\nTherefore, all else equal, she is much more likely to inspect it. This e\u00a4ect is further magni\u0085ed\nwhen she expects to share it with other fellow right-wingers, because they are also more likely\nto inspect a left-wing message, and if it contains misinformation, it is likely to be found out,\nwith costly implications for our focal agent.\nIn contrast, consider this right-wing agent receiving a right-wing message. Now she is less\nsuspicious of the message, and all else being equal, she will use a lower threshold \u0097 this means\nthat she will choose a lower threshold \u0016rsuch that she inspects it only if the reliability score is\nr <\u0016r. Homophily now has the opposite e\u00a4ect: if she is in a homophilic network, she expects\nother right-wingers not to inspect the news item either, and as a result, she is less likely to\ninspect it herself. When an item is not inspected, then it is more likely to become viral \u0097\neverybody starts sharing it.\nThese observations lead to the \u0085rst basic result in the setup: a news item is more likely\nto become viral when (i) it reaches individuals who have congruent beliefs, and (ii) when this\nconcordant news item is being shared with others with similar beliefs.\nNow consider the problem of the platform on which this community is situated. Suppose\n33",
        "04fcb7c8-b9a9-4afc-805d-01d52e92658f": "that, via its choice of algorithms, the platform in\u0087 uences the degree of homophily (as in\nAcemoglu, Ozdaglar and Siderius, 2021). Suppose also that the platform\u0092 s objective is to\nmaximize engagement and hence, greater virality of the article is better for the platform. First\nsuppose that most of the news items arriving from the outside have high reliability scores\nand are also being distributed roughly equally between the two sub-communities (so that it is\nnot only left-wing messages going to the left-wing community and likewise for the right-wing\ncommunity). Then the platform\u0092 s engagement-maximizing policy is likely to be to introduce\nbetween-community links, thus exposing each individual to news items from the other side,\nsince these are all reliable and thus unlikely to be killed, even if they were inspected.\nIn contrast, suppose we have a situation in which there are many low-reliability news\nitems, and moreover left-leaning news items from the outside go to the left-wing community\nand right-leaning news items go to the right-wing community. If the platform were interested\nin stopping misinformation, it could choose low homophily (so that right-wing articles go to the\nleft-wing community as well, for example), and this would induce less sharing among the right-\nwing agents (expecting inspection from the left-wingers). It would also cause interruptions to\nvirality when left-wingers discover the right-wing news item to contain misinformation (which\nis likely since it has low-reliability). These considerations imply that when news items have\nlower reliability, the platform would prefer to induce extreme homophily via its algorithms and\npropagate misinformation in order to maximize engagement.\nThis result, mimicking the main \u0085nding of Acemoglu, Ozdaglar and Siderius (2021), is in\nsome ways quite striking. It highlights that the platform has incentives to create endogenous\necho chambers (or \u0085lter bubbles). Worse, this happens precisely when there are low-reliability\nnews items likely to contain misinformation and distributed within the community in a \u0093po-\nlarized fashion\u0094(left-wing messages going to left-wing groups, etc.).\nThe role of AI technologies is again crucial. Without these technologies, the platform\nwould not be able to determine users\u0092biases and create relatively-homogeneous communities.\nIt would also not be able to support the rapid dissemination of viral news items.\nBroader social implications of these types of \u0085lter bubbles are easy to see as well. Suppose\nthat individuals also update their beliefs on the basis of the news items. When left-wingers\nreceive right-wing news items and the relevant news items have reasonable reliability scores,\nthey will tend to moderate their beliefs, exactly as Sunstein (2001) envisaged in the quote\nabove. In contrast, when left-wingers receive only left-wing news items in a \u0085lter bubble, this\nmight lead to further polarization. On the basis of these considerations, Acemoglu, Ozdaglar\n34",
        "944b16c2-aeb8-4dc6-bf48-7238b396efc8": "and Siderius (2021) suggest various interventions, including regulation and outside inspections,\nin order to discourage such \u0085lter bubbles and reduce polarization, and I return to the issue of\nregulation in Section 6.\nIn summary, this section leads to the following general lessons:\n1. AI-powered social media presents a variety of new opportunities for connecting individ-\nuals and information sharing.\n2. However, in the process it may also distort individuals\u0092willingness to share unreliable\ninformation. When social media creates echo chamber-like environments, in which indi-\nviduals are much more likely to communicate with like-minded others, they become less\ncareful in inspecting news items that are consistent with their existing views and more\nwilling to allow the circulation of misinformation.\n3. Centrally, social media platforms that are focused on maximizing engagement have an\nincentive to create echo chambers (or \u0093\u0085lter bubbles\u0094 ), because inspection and interrup-\ntions of the circulation of news items with unreliable messages reduces engagement. As\na result, especially when there are more items with misinformation, platform incentives\nare diametrically opposed to social objectives.\n4.2 Perils of Online Communication\nThe previous subsection argued that political discourse may be hampered because of the algo-\nrithmic policies of digital platforms. In this subsection, I suggest that there might be reasons\nmore endemic to the nature of online communication that disadvantages political communi-\ncation (see also Lanier, 2018; Tirole, 2021). The main argument is a version of the ideas\ndeveloped in Acemoglu, Kleinberg and Mullainathan (2022), and here I will provide a simple\nillustration.\nConsider \u0085rst a pre-AI world in which large-scale social media communication is not pos-\nsible. Suppose that in such a world, individuals communicate bilaterally in a social network.\nFor simplicity, we can imagine a social network that takes the form of a directed line, in which\nwe start with individual 0, who can communicate with individual 1, and all the way up to\nthe end of the line, individual n. Suppose that each individual has a piece of gossip which\nthey can share with their neighbor, which will give utility vG. In addition, the individual may\nhave some news relevant for the political/social beliefs of the community. If she decides to\n35",
        "69c91987-002b-4bd6-a69c-dd21d9039a00": "share this and this news item alters the beliefs of her neighbor, she also receives utility from\nsuch persuasion, as I specify below (for simplicity, I am ignoring potential utility bene\u0085ts from\nnon-neighbors indirectly changing their beliefs as this information is shared further). Because\nthere are \u0093channel constraints\u0094(for example, limited ability to communicate), the individual\ncan either share political news or gossip, but not both.\nSpeci\u0085cally, suppose that the state of the world is 0 or 1 (Left or Right), and all individuals\nstart with prior \u00160=1\n2about the underlying state being s= 0. Suppose that individual 0\nreceives a piece of news that shows that the underlying state is in fact s= 0. If she shares this\ninformation with her neighbor (individual 1) and her neighbor believes it, then her neighbor\u0092 s\nbelief will also shift to \u0016= 1. However, each individual is also concerned that some agents in\nsociety may have ulterior motives and try to convince them that the state is the opposite of\nthe true state s. Suppose the probability that each individual attaches to their neighbor being\nof this extremist type is q. Then the posterior of individual 1 that the state is s= 0after\nreceiving this news will be\n\u00161=1\n2\n1\n2+1\n2q=1\n1 +q>1\n2:\nFinally, I assume that individuals receive additional utility from shifting their neighbor\u0092 s\nbeliefs towards the truth (or her own belief), so the overall utility of individual iis\nvGxG\ni+vN\f\f\u0016i+1\u0000\u0016i\f\f;\nwherexG\ni= 1denotes whether this individual gossips, \u0016iis her belief, and \u0016i+1is her neighbor\u0092 s\nbelief (given the line network).\nLet us assume that vN>2vG, so that if an individual is convinced that her information\nwill be believed and can thus shift her neighbor\u0092 s belief from 1=2to her views, she prefers to\nshare the political information rather than gossip. Therefore, there exists \u0016qsuch that if q<\u0016q,\nthe individual will share the political news. Let us suppose that in the in-person social network\nthis inequality is satis\u0085ed.\nIn this scenario, individual 0 will start sharing the political news. This will convince in-\ndividual 1, who will then attach a su\u00a2 ciently high probability to the underlying state being\ns= 0. With the same argument, she would also like to convince her neighbor to the right, in-\ndividual 2. If qis su\u00a2 ciently small, individual 2, even when she is worried about the possibility\nthat either individual 0 or individual 1 being an extremist, would still believe this information.\nIn general, there will exist some n(q), such that the political news will be communicated up to\n36",
        "6a3edb5b-2e91-494e-bec4-f3adff629d86": "individualn(q), and then after that there will be pure gossip on the network. For qsu\u00a2 ciently\nsmall, the entire network might share the political news.\nNext suppose that we go to online communication. This leads to a larger network and less\npersonal contact. As a result, it is plausible to assume that the probability that each agent\nattaches to the event that the person communicating with them is an extremist is now higher,\nsayq0> q. Ifq0>\u0016q, then in online communication, there will be no news exchange and all\ncommunication will be gossip.\nThe situation may be worse when we take into account that online interactions typically\ntake the form of broadcast (rather than bilateral) communication. This would exacerbate the\nsituation I have outlined here for two reasons. First, there may be many agents who may\nwant to broadcast their views, and the same \u0093channel constraints\u0094 would imply that only\none of them can do so, and broadcasting may be particularly attractive to extremists. This\nwould endogenously increase the assessment of all the agents that political news is coming\nfrom extremists. Second, if there is heterogeneity in the utility of gossip across agents, those\nwho value gossiping most may be the ones monopolizing the channels. In both cases, online\ncommunication becomes less e\u00a4ective as a way of sharing politically or socially relevant infor-\nmation. If political communication and news sharing in social network is an important aspect\nof democratic politics, then the forces identi\u0085ed in the subsection also create new challenges\nfor democracy, again rooted in the use of AI technologies.\nThe general lessons from this brief analysis are simple as well:\n1. Bilateral, o\u00a4-line communication, especially when the subject matter is political or social,\nrelies on trust between parties. Naturally-existing trust in in-person social networks may\nenable this type of communication.\n2. When communication is taking place online and in multi-lateral settings, such as in\nmodern social media platforms powered by AI technologies, this type of trust-based\ncommunication becomes harder. This may favor non-political messages, such as gossip,\nwhich then drive out political communication, with potentially deleterious e\u00a4ects for\npolitical discourse and democracy.\n3. This potential barrier to online communication is exacerbated when there is competition\nfor attention, which is encouraged by the broadcast or multi-lateral nature of online\ncommunication.\n37",
        "8628f9a9-8e6f-4125-ac27-1be0beec0a2a": "4.3 Big Brother E\u00a4ects\nThe previous two subsections focused on how AI-powered social media and online platforms\nchange the nature of communication, with potentially negative e\u00a4ects on the sharing of political\ninformation, which is the bedrock of democratic participation by citizens. In this subsection, I\nsuggest that the other crucial pillar of democratic institutions, citizen protests, is likely to be\nhampered by AI technologies.\nI have argued in Acemoglu and Robinson (2000, 2006) that protests, riots and uprisings\nare critical for the emergence of democratic regimes (because the threats that they pose for\npower-holders in nondemocratic regimes induces democratization). This argument is relevant\nfor democratization in currently authoritarian governments, such as China, Russia or Iran.\nIn particular, if AI-based monitoring of communication and political activity extinguishes\npolitical dissent and makes it impossible for opposition groups to organize, the longevity of\nnondemocratic regimes may increase signi\u0085cantly.\nThe problem is not con\u0085ned to these nondemocratic nations, and applies to the US and\nother countries with democratic institutions as well. A similar argument suggests that protests\nand civil disobedience are often critical for the functioning of democratic regimes as well. The\ncivil rights movement in the US illustrates this vividly. Even though the US was democratic\nat the federal level, the Jim Crow South routinely violated the political, social and economic\nrights of Black Americans. Democratic institutions in the North and, to the extent that that\nthey existed in the South, did not create a natural impetus for these discriminations to cease.\nThe turning point came with civil disobedience organized by various Black (and later multi-\nethnic) civil society groups, such as the NAACP. Vitally, even federal politicians opposed to\nJim Crow were not in favor of these protests initially, viewing them as disruptive and a political\ndrawback for them, especially given that any federal action against Jim Crow practices would\ntrigger backlash from Southern politicians (see the discussion in Acemoglu and Robinson,\n2019). Without civil disobedience, protest and other sources of bottom-up pressures, it is\nlikely that reform of voting, civil, education and discrimination laws in the US South would\nhave been further delayed.\nHere I brie\u0087 y outline a simple model capturing some of these ideas. Consider a society\nconsisting of \u0015 < 1=2elites and measure 1of regular citizens. All citizens and all elites\nhave the same economic preferences, but citizens are heterogeneous in terms of their cost of\nparticipating in protest activities, denoted by cifor individual i. I assume that cis distributed\nuniformly over [0;1]in the population.\n38",
        "a9ee51cb-3350-4585-a413-d26374f8049d": "The political system is an imperfect democracy or an autocracy in which political choices\nare biased in favor of the preferences of the elite. In particular, suppose that there is a unique,\none-dimensional policy, and the preferred policy choice of the citizens is 0, while the most\npreferred policy of the elite is pE>0. Consider the following reduced-form political game.\nThe elite decide a policy p, and then protests take place. If a fraction qof the citizens protest\nand engage in civil disobedience, then there is probability \u0019(q)that the policy will switch from\npto0. With the complementary probability, the policy stays at p. This political structure\nignores the in\u0087 uence of the citizens via democratic institutions, which is for simplicity. If this\nis incorporated, for example, as in Acemoglu and Robinson (2008), this would not a\u00a4ect the\nmain message of the model presented here.\nI also assume that the government can impose a punishment on those engaged in protests.\nSuppose, in particular, that the state has the capacity to detect at most a measure  of\nprotesters. If the total amount of protest, q, is less than  , then all protesters are detected\nand can be punished. I assume that the punishment imposed on protesters is a constant, \u0000,\nindependent of the number of protesters.\nThe two key economic decisions are therefore policy choice by elites and protests by citizens.\nLet me \u0085rst describe the utility of the citizens. Suppose that when the policy choice of elites\nispand there are qprotesters in total, individual ihas the following utility as a function of\nher protest decision xi2f0;1g:\nUC\ni(p;q;xi) =\u0014\n(vCjpj\u0000ci)\u0000min\u001a \nq;1\u001b\n\u0000\u0015\nxi;\nwhere, for centricity, I have ignored components of the utility that depend on policy choice but\nare independent of the individual\u0092 s process decision. Intuitively, the utility from protesting is\nincreasing in the distance between the actual policy and the bliss point of citizens, as captured\nbyvCjpj(recall that their bliss point is at zero). In addition, the individual incurs the cost\nof participating in protests, given by ci. The second term in square brackets captures the\nexpected punishment from protesting, taking into account that when q\u0014 , protesters will be\npunished with probability 1.\nClearly, there exists a threshold value \u0016c, such that only individuals with ci\u0014\u0016cwill partic-\nipate, and thus q= \u0016c, since the distribution of cis uniform between 0 and 1.\nLet us next turn to the elite\u0092 s utility. Suppose that this is given by\nUE(p;q) =\u0000E\u0002\f\f^p\u0000pE\f\f\u0003\n=\u0000(1\u0000\u0019(q))vE\f\f^p\u0000pE\f\f\u0000\u0019(q)vE\f\fpE\f\f;\n39",
        "23e647eb-bcf9-4126-a6e8-81d645387e2c": "where ^pdenotes the realized policy and the expectation is over the uncertainty concerning\nwhether protests will force the elites to change policy. As usual, the subgame perfect equi-\nlibrium can be solved by backward induction. In the second stage, \u0016cis determined such that\ngiven the policy choice of elites, p, we have:\n(vCjpj\u0000 \u0016c(p))\u0000min\u001a \n\u0016c(p);1\u001b\n\u0000 = 0: (8)\nIn general, there can be multiple equilibria in this stage, because this equation might have\nmultiple solutions for \u0016c(p). Note in particular that its left-hand side may be non-monotonic.\nIn what follows, I focus on the case in which it is monotonically decreasing, which ensures\na unique equilibrium (if there are multiple equilibria, we could pick the one with the highest\namount of protest, which will necessarily be one where the left-hand side is decreasing, yielding\nthe same results). It is then straightforward to see that \u0016c(p)is increasing in p, meaning that\na more pro-elite policy induces more protests.\nNow turning to the elite\u0092 s maximization, we \u0085rst rewrite the elite\u0092 s utility function taking\ninto account the reaction of the citizens to their policy choice:\nUE(p;\u0016c(p)) =\u0000(1\u0000\u0019(\u0016c(p)))\f\fp\u0000pE\f\f\u0000\u0019(\u0016c(p))\f\fpE\f\f;\nwhich simply substitutes q= \u0016c(p). We can therefore maximize elite utility by choosing the\ninitial policy p. Taking into account that p<pE, this maximization problem yields a standard\n\u0085rst-order condition:\n(1\u0000\u0019(\u0016c(p)))\u0000\u00190(\u0016c(p))\u0016c0(p)(2pE\u0000p) = 0; (9)\nand under suitable assumptions, we can ensure that the second-order condition for maximiza-\ntion is satis\u0085ed. In this \u0085rst-order condition, \u0016c0(p)is given from (8), and under the assumption\nthat <\u0016c(p), it can be written as\n\u0016c0(p) =vC\n1 + \u0000=\u0016c(p)2:\nConsider next the introduction of AI, modeled as an increase in  to some 0. From the\nprevious expression, this increase will reduce \u0016c0(p), and from (9), this reduction in \u0016c0(p)will\nincreasepaway from the citizens\u0092bliss point and towards the elite\u0092 s preferences. Intuitively, AI-\ninduced government monitoring of protests weakens citizens\u0092ability to force the elites to make\nconcessions, and the elite respond to the deployment of the AI technology by withdrawing\nconcessions. As a result, AI makes policies less responsive to citizens\u0092 wishes, and to the\n40",
        "57274488-55cf-480c-a0cc-da7c5723dec3": "extent that these policies impact the distribution of resources, it will also tend to favor the\nelite\u0092 s economic interests and increase inequality.\nOverall, the general lessons from this simple model are:\n1. AI technologies can be used for improving government monitoring against protest activ-\nities.\n2. Since the threat of protests has a disciplining role on nondemocratic governments, and\neven on some democratic governments, the shift of power away from civil society towards\ngovernments will weaken democracy and aggravate policy distortions.\n4.4 Automation, Social Power and Democracy\nThe previous subsection explained how the use of AI as a tool for controlling society and\npolitical dissent can have harmful e\u00a4ects on democratic politics. In this subsection, I argue\nthat AI-powered automation can further weaken democracy and undermine social cohesion.\nTo develop this argument, let us \u0085rst go back to the framework in Acemoglu and Robinson\n(2006), which emphasizes two types of conditions for the emergence and survival of democratic\ninstitutions. First, there must be enough discontent with nondemocratic regimes to generate\na demand for democracy. Second, democracy should not be too costly for the elite, who would\notherwise prefer repression or other means to avoid sharing political power with the broader\npopulation. One aspect of this problem, which could be important but was not analyzed in\nAcemoglu and Robinson (2006), is cooperation from workers. For example, if workers become\ndisenchanted with the current regime or decide that they need to take action against current\n(economic or political) power-holders, they may choose not to cooperate with capital in their\nworkplaces. When labor is essential for production, this withdrawal of cooperation could be\nvery costly for capital. When capital owners are in\u0087 uential in the political system, they may\nthen push for democratization or redistribution in order to placate labor.\nThe main argument in this subsection is that automation may make workers less indispens-\nable in workplaces, and as such, it will tend to reduce their political power.\nLet us return to the model in Section 3.1 and for simplicity, suppose that N= 1and\u001b= 1\nin the production function (3) and also assume that labor markets are competitive (there is no\nwage \u0087 oor). This implies that the equilibrium level of production as a function of capital and\nlabor can be written as\nY(K;L) =KIL1\u0000I,\n41",
        "fc97c01b-2d8b-46c1-9b93-d125e26d3dd7": "and thus with competitive labor markets, the labor share is sL= 1\u0000I, and\nw= (1\u0000I)Y(K;L)\nL:\nNote also that in this case the impact of automation on output can be written as\n@lnY(K;L)\n@I= lnK\u0000lnL.\nTherefore, automation is output-increasing, when lnK > lnL(orK >L, which is also equiva-\nlent to the competitive rental rate of capital being less than the wage, ensuring that automation\nis cost-saving). Conversely, low-productivity(\u0093so-so\u0094 ) automation now corresponds to the case\nin whichK\u0019L.\nConsider a political system as in Acemoglu and Robinson (2006), where all capital owners\n(capitalists) are elites and all workers are non-elites, with no within-group heterogeneity. To\nstart with, let us consider a nondemocratic regime in which the capitalists hold power, or\nalternatively a democratic regime in which they have disproportionate power. For brevity, I\nam going to ignore any threat of revolution or protests along the lines of the models considered\nin Acemoglu and Robinson (2006) and will also abstract from the considerations discussed in\nthe previous subsection.\nSuppose in addition that there is a lump-sum tax on capitalists, which can be redistributed\nto workers, and let us denote the per-worker transfer on the basis of this by \u001c. Suppose that\nthe workers have an aspiration for a level of net income given by wA, and ifw+\u001c < wA,\nthey withdraw their cooperation, and as a result, the e\u00a4ective productivity of labor declines\nfrom 1to\u000e <1. In this reduced-form model, I interpret the transfer \u001cfrom the elite both\nas a measure of redistributive politics and also as a general concession to democratic politics\n(for example, allowing workers to have more voice or making less use of lobbying and other\nactivities in order to distort democratic politics).\nThe key question is whether the elite will make the necessary transfers to convince workers\nto continue cooperating in workplaces. This boils down to the comparison of the following two\noptions for the elite: redistribute via \u001cso that the aspirations of the workers are met, or make\ndo with lower labor productivity due to lack of cooperation. Let us write the payo\u00a4s to the\nelite from these two strategies. Suppose there is no within-elite heterogeneity, so it is su\u00a2 cient\nto look at the overall income of capital. When the capitalists choose to meet the aspiration\nconstraint of workers, their payo\u00a4 is\nUK\n1=KIL1\u0000I\u0000max\b\n(1\u0000I)KIL1\u0000I;wAL\t\n;\n42",
        "3b8ed764-cc57-4e68-82df-2d9e2b913f4e": "where the \u0085rst term in the max operator applies when the market wage is already greater\nthanwA, while the second term is when they have to make transfers in order to bring workers\nto this level. Clearly, the relevant case for the discussion here is the latter, so I assume that\n(1\u0000I)KIL1\u0000I<wAL, and thus\nUK\n1=KIL1\u0000I\u0000wAL:\nThe alternative is not to make the necessary transfer, in which case the elite simply receive\nthe market return to capital when the productivity of labor has been reduced to \u000e, i.e.,\nUK\n2=KI(\u000eL)1\u0000I:\nWhat is the e\u00a4ect of automation on the comparison of these two strategies? Di\u00a4erentiating\nthe di\u00a4erence in their payo\u00a4s, UK\n1\u0000UK\n2, with respect to Iyields\n@(UK\n1\u0000UK\n2)\n@I<0if and only if lnK\u0000lnL+ln\u000e\n1\u0000\u000e1\u0000I<0:\nThis expression has two immediate implications. First, if we have low-productivity automa-\ntion, so that lnK\u0019lnL, then because ln\u000e<0, automation always makes the second strategy\nof stopping redistribution and foregoing labor\u0092 s cooperation more attractive. Intuitively, au-\ntomation makes labor less central for production, and thus losing its cooperation becomes less\ncostly for capital. The economic force going against this calculation is that when automation\nincreases productivity (when lnK > lnL), the output loss due to lack of cooperation from\nlabor becomes more costly. Second, however, we can also see that, for any KandL, there\nexists a threshold level I\u0003such that once I >I\u0003, the second strategy is preferred, even taking\ninto account the productivity gains from automation. Therefore, automation makes cooper-\nation from workers less important, and to the extent that securing this cooperation was an\nimportant part of the motivation for redistribution and democratic politics, automation may\nmake elites turn their back on or even become hostile to democracy.\nIn summary, this subsection has established:\n1. Automation can also generate an indirect negative impact on democracy and redistribu-\ntive politics when ensuring cooperation from labor in workplaces is an important moti-\nvation for elites to make concessions to labor.\n2. When automation brings only small productivity gains, it encourages the elite to re-\nduce redistribution and make fewer democratic concessions. This will make policies less\nresponsive to the majority\u0092 s wishes and may further raise inequality.\n43",
        "ad13a8aa-9dde-4edf-b76b-ce9f94f52617": "3. Productivity bene\u0085ts of automation may soften this e\u00a4ect, because an automation-driven\nincrease in output raises the opportunity cost of losing labor\u0092 s cooperation. Nevertheless,\nthere exists a su\u00a2 ciently high level of automation such that once we reach this level, labor\nbecomes su\u00a2 ciently irrelevant for production that the withdrawal of their cooperation\nceases to be very costly. After this threshold, the elite may prefer to abandon democratic\ninstitutions and withhold any concessions, and proceeds without labor\u0092 s cooperation,\nonce again with harmful e\u00a4ects on democracy, redistribution and social cohesion.\n5 Other Potential Costs\nIn this section, I brie\u0087 y list a few other areas that may be important, but without providing\nas much detail or any formal analysis.\nThe threat of AI and bargaining: Even when the actual labor market e\u00a4ects of AI discussed\nin Section 3 are not realized, the threat of adopting AI technologies may in\u0087 uence wages and\ninequality. Speci\u0085cally, if there is bargaining and rent-sharing, employers may use the threat\nof AI-based automation as a way of increasing their bargaining power, which could have some\nof the same e\u00a4ects as actual automation and AI-powered monitoring.\nDiscrimination: Bias in AI has already received considerable attention. As AI gains greater\nimportance in our social and economic life, ensuring that popular algorithms are fair and\nunbiased has become vital. Existing studies show that simple AI algorithms can improve\nimportant public decisions, such as bail or sentencing, without increasing discrimination (e.g.,\nKleinberg et al., 2018). However, in most such applications, algorithms use data generated from\nbiased agents and potentially discriminatory practices (e.g., Thompson, 2019). For example,\nboth the police and the legal system in the US are generally thought to be biased against certain\ngroups, such as Black Americans. In such situations, there is a danger that these biases will\nbecome a fundamental part of AI algorithms. This may not only promote persistent bias\nand discrimination, but may in fact cement these biases more deeply in society via a process\nsimilar to the \u0093signaling role of laws\u0094 (e.g., Posner, 2002). Indeed, if society starts trusting\nAI algorithms, their discriminatory choices may come to be accepted as more justi\u0085able than\nwhen they were made by individual decision-makers.\nTechnocracy versus democracy: Advances in AI may create the temptation to delegate more\nand more public and even political decisions to algorithms or to technocrats designing and using\n44",
        "9187835b-cc3c-4d0b-beb1-fa95431af9cd": "these algorithms. Although this may be justi\u0085able for certain decisions, excessive reliance on\ntechnocracy, without citizen input, may also start encroaching into political decisions \u0097 such\nas the extent of redistributive taxation or how much we should protect disadvantaged groups\n(Sandel, 2020). In this case, reliance on AI may further undermine democracy, amplifying the\nconcerns highlighted in the previous section.\nAI-powered weapons: AI technologies have already started being incorporated into weapons\nand are advancing towards autonomous weapon systems. These new technologies will cause\na host of ethical and social dilemmas, and may need to be regulated before prototypes are\ndeployed or even fully developed. In addition to these ethical and social issues, AI-powered\nweapons may further strengthen governments against civil society, protesters and even some\nopposition groups, adding to the concerns we discussed in the previous section.\nThe alignment problem: The potential downside of AI technologies that has received most\nattention is the \u0093alignment problem\u0094 : the problem of ensuring that intelligent machines have\nobjectives that are aligned with those of humanity. Many researchers and public intellectuals\nare concerned about machines reaching super-human capabilities and then implicitly or explic-\nitly turning against humans (e.g., Bostrom, 2014, Russell 2019, Christian, 2019). Although my\nown view is that these concerns are somewhat overblown and often distract from shorter-term\nproblems created by AI technologies (on which this essay has focused), they deserve careful\nconsideration, monitoring and preparation.\nThe international dimension: The current development of AI technologies is intertwined\nwith international competition, especially between the US and China (Lee, 2018). A discussion\nof regulation of AI has to take into account this international dimension. For example, it\nmay not be su\u00a2 cient for the US and Europe to start regulating the use of data or excessive\nautomation, when they remain almost completely unregulated in China. This suggests that\nthe regulation of AI needs to have a fully-\u0087 edged international dimension and we may need to\nbuild new international organizations to coordinate and monitor deregulation of AI across the\nglobe.\n6 The Role of Technology Choice and Regulation\nIn the preceding sections, I went through a number of theoretical arguments suggesting that\nthe deployment of AI technologies may generate economic, political and social costs. In this\n45",
        "975fd742-824b-496f-a8fc-321f65b47d6e": "section, I highlight that in all of these cases the problems are not inherent to AI technologies\nper se. Rather, the harms I have emphasized are caused by corporate and societal choices on\nhow these technologies are deployed. Even though these costs are far-ranging, taking place\nin product markets, in labor markets and in the realm of politics, they exhibit a number of\ncommonalities, which I explore in this section. I also discuss some possible remedies. The\ngeneral emphasis in this section will be on three main ideas:\n1. The importance of choices, both on how existing AI technologies are used and on the\ndirection of AI research. The costs I have modeled are not in the nature of AI technolo-\ngies, but depend on how this new technological platform is being developed to empower\ncorporations and governments against citizens and workers.\n2. The inadequacy of market solutions that mainly rely on increasing competition.\n3. The need for regulation.\nLet me start with the mechanisms discussed in Section 2. All three of these potential costs\nof AI turn on how AI technologies enable the use and control of data. In each one of these\ncases, a di\u00a4erent way of distributing control rights over data would ameliorate or prevent most\nof the costs (Posner and Weyl, 2019). Let us start with the model of data markets in Section\n2.1. The source of ine\u00a2 ciency in this case is the ability of platforms to \u0085nd out information\nabout others from the data that an individual shares. This then opens the way to potential\nmisuses of data, for example in order to reduce the surplus of consumers or by violating their\nprivacy in other ways. E\u00a4ective regulation in this case could take one of two forms. First, as\nsuggested in Acemoglu et al. (2021), it may be possible to strip away parts of the data of an\nindividual in order to prevent or minimize information about others being leaked (though the\ndetails matter here, and just anonymizing data would not be useful). Second, more systematic\nregulations on how platforms can utilize the information they acquire would lessen the harmful\ne\u00a4ects working through privacy.\nIn contrast, increasing competition may not be su\u00a2 cient, and not even useful, in this case.\nMy analysis in Section 2.1 focused on a monopoly platform. Acemoglu et al. (2021) show\nthat if there are two platforms that are competing to attract users, this may exacerbate the\npernicious e\u00a4ects of data externalities.\nLet me then turn to the model considered in Section 2.3. The source of ine\u00a2 ciency in this\ncase is the ability of platforms to use the data individuals reveal about themselves in order to\n46",
        "c0a54865-fd5f-48bf-ba29-3ebaaafb1033": "manipulate their weaknesses. If this misuse of data can be prevented or if consumers can be\nmade more aware of how data are being used, some of these costs could be prevented. Suppose,\nfor example, that consumers are informed frequently that platforms know more about their\npreferences and sometimes market products that are bad for them. There is no guarantee that\nsuch informational warnings will work for all consumers, but if they are displayed saliently and\nare speci\u0085c (for example, calibrated according to the group of individuals and relevant class of\nproducts), they may prevent some of the harms identi\u0085ed by the analysis above. In this case,\ntoo, increasing competition would not be an e\u00a4ective solution. If two platforms are competing\nfor consumers, but consumers continue to be semi-behavioral and fail to recognize the increase\nin platform capabilities, both platforms may try to exploit their ability to o\u00a4er products that\nhave short-term apparent bene\u0085ts and long-term costs.\nThe issues are similar when we turn to the economic forces studied in Section 2.2, but\nnow the implications of competition are more nuanced. In this case, e\u00a4ective regulation would\nprevent one of the \u0085rms from using the additional information it acquires for capturing all of\nthe consumer surplus. Price controls and limits on price discrimination might be some of the\nmethods for achieving this, though clearly such regulation is far from straightforward. What\nhappens if we can increase competition in this case? Greater competition that results from\n\u0085rm 0 also using AI methods to estimate its own past consumers\u0092preferences and customize its\nservices accordingly would not be necessarily useful. Now both \u0085rms become local monopolists,\ncapturing all of the consumer surplus. However, if each \u0085rm can also acquire information about\nthe other\u0092 s customers and collusion can be prevented, then they can be induced to compete\n\u0085ercely, from which consumers might bene\u0085t. This case thus highlights that in some scenarios\nfostering competition might have bene\u0085ts, though even in this instance, it can only do so to a\nlimited extent and only when some case-speci\u0085c conditions are satis\u0085ed.\nI would also like to emphasize the implications for the direction of AI research in this case.\nSuppose that AI researchers can devote their time in order to develop alternative applications\nof this broad technological platform. For example, some of them may be able to use AI to\ncreate tools that empower citizens or consumers, or develop new technologies for preserving\nprivacy. All the same, if any one of the mechanisms related to the control and misuse of\ninformation are relevant, then this will also produce a powerful demand for technologies that\nenable corporations to acquire and better exploit this type of information. This is more so\nwhen the ability of consumers to pay for alternative technologies is limited relative to the\nresources in the hands of corporations. In such scenarios, the demand for \u0093misuse of AI\u0094will\n47",
        "877ff170-4a87-4437-a6dd-d14e38a6c58f": "be transmitted to AI researchers, who may then respond by devoting their time to developing\nthe AI technologies that corporations need and by moving further away from technologies that\nmay have greater social value or empower consumers and citizens. This is a general point,\nwhich applies whether the harmful e\u00a4ects of AI are on the control of information, in labor\nmarkets or in the context of politics. It is for this reason that innovation, when it is itself\nunregulated, is unlikely to produce self-correcting dynamics. On the contrary, the demand\nfor misuse of AI will typically distort the allocation of research across di\u00a4erent applications,\namplifying its social and economic costs.\nThe same considerations apply even more evidently in the models I discussed in Section 3.\nIf automation is excessive, increasing competition in the labor market would not be particularly\nuseful. On the other hand, the demand for automation technologies from \u0085rms will tend to\nbe strong, encouraging researchers to double down on using AI for developing automation\ntechnologies. Regulatory solutions are again feasible, but may be more di\u00a2 cult to design and\nimplement in this case. In theory, when automation is excessive and AI research is not being\ndirected to creating new tasks, welfare-promoting regulation should discourage automation at\nthe margin and encourage the creation of new labor-intensive tasks.\nHowever, distinguishing marginal (low-productivity) and infra-marginal (higher-\nproductivity) automation is di\u00a2 cult. Even more challengingly, regulators might have a hard\ntime separating AI used for creating new tasks from AI being used for automating low-skill\ntasks while empowering higher-skilled or managerial workers. But it is also possible to view\nthese problems not as absolute barriers but as measurement challenges. More research might\nshed light on how to distinguish di\u00a4erent uses of AI in the labor market and might reveal new\nregulatory approaches for in\u0087 uencing the direction of AI research.\nFinally, there are similar lessons from the models we discussed in Section 4, though there\nare also some new challenges related to the fact that the e\u00a4ects are now on political and\ndemocratic outcomes. For one, increasing competition is unlikely to be a very e\u00a4ective way of\ndealing with misaligned platform incentives. For example, if there are multiple social media\nplatforms trying to maximize engagement, each may have incentives to create \u0085lter bubbles.\nPro-competitive solutions may also be less e\u00a4ective when there are systemic issues, such as the\nwidespread malfunctioning of democratic institutions.\nThe e\u00a4ects of these new technologies for democratic politics raises new conceptual issues as\nwell. Most importantly, if the incorrect deployment of AI technologies is weakening democratic\npolitics, developing after-the-fact regulatory solutions might become harder, since democratic\n48",
        "323d83bf-566e-43ad-9e1a-7af2d8b97f6f": "scrutiny of those who bene\u0085t from the distortionary use of AI technologies would also becomes\nmore di\u00a2 cult. These considerations then suggest a \u0093precautionary regulatory principle\u0094\u0097 ex\nante regulation slowing down the use of AI technologies, especially in domains where redressing\nthe costs of AI become politically and socially more di\u00a2 cult after large-scale implementation.\nAI technologies impacting political discourse and democratic politics may be prime candidates\nfor the application of such a precautionary regulatory principle.\n7 Conclusion\nIn this essay, I explored several potential economic, political and social costs of the current\npath of AI technologies. I suggested that if AI continues to be deployed along its current\ntrajectory and remains unregulated, then it can harm competition, consumer privacy and\nconsumer choice, it may excessively automate work, fuel inequality, ine\u00a2 ciently push down\nwages, and fail to improve productivity. It may also make political discourse increasingly\ndistorted, cutting one of the lifelines of democracy. I also mentioned several other potential\nsocial costs from the current path of AI research\nI should emphasize again that all of these potential harms are theoretical. Although there\nis much evidence indicating that not all is well with the deployment of AI technologies and\nthe problems of increasing market power, disappearance of work, inequality, low wages, and\nmeaningful challenges to democratic discourse and practice are all real, we do not have su\u00a2 cient\nevidence to be sure that AI has been a serious contributor to these troubling trends.\nNevertheless, precisely because AI is a promising technological platform, aiming to trans-\nform every sector of the economy and every aspect of our social lives, it is imperative for us\nto study what its downsides are, especially on its current trajectory. It is in this spirit that I\ndiscussed the potential costs of AI this paper.\nMy own belief is that several of these costs are real and we may see them multiply in the\nyears to come. Empirical work exploring these issues is therefore greatly needed.\nBeyond empirical work, we also need to understand the nature and sources of these potential\ncosts and how they can be prevented. It is for this reason that I suggested various policy\nresponses, in each case emphasizing that the costs are rooted in the way that corporations and\ngovernments are choosing to develop and use these technologies. Therefore, my conclusion is\nthat the best way of preventing these costs is to regulate AI and redirect AI research, away\nfrom these harmful endeavors and towards areas where AI can create new tasks that increase\n49",
        "86ba3d86-7356-4094-acd4-7e4e41c9a055": "human productivity and new products and algorithms that can empower workers and citizens.\nOf course, I realize that such a redirection is unlikely, and regulation of AI is probably more\ndi\u00a2 cult than the regulation of many other technologies \u0097 both because of its fast-changing,\npervasive nature and because of the international dimension. We also have to be careful, since\nhistory is replete with instances in which governments and powerful interest groups opposed\nnew technologies, with disastrous consequences for economic growth (see, e.g., Acemoglu and\nRobinson, 2012). Nevertheless, the importance of these potential harms justi\u0085es the need to\nstart having such conversations.\n50",
        "4bbe3409-9ff8-49fe-99c1-dc00d3929e83": "References\nAbramowitz, Alan I. (2010) The Disappearing Center: Engaged Citizens, Polarization,\nand American Democracy . New Haven, CT: Yale University Press.\nAcemoglu, Daron (2002) \u0093Technical Change, Inequality, and The Labor Mar-\nket,\u0094Journal of Economic Literature , 40(1): 7\u0096 72.\nAcemoglu, Daron (2019) \u0093It\u0092 s Good Jobs Stupid,\u0094 Economics for Inclusive Prosperity ,\nResearch Brief, June 2019. https://econ\u0085p.org/wp-content/uploads/2019/06/Its-Good-Jobs-\nStupid.pdf\nAcemoglu, Daron (2021) \u0093AI\u0092 s Future Doesn\u0092 t Have to Be Dystopian,\u0094 Boston Review ,\nMay 20, 2021. http://bostonreview.net/forum/science-nature/daron-acemoglu-redesigning-ai\nAcemoglu, Daron and David H. Autor (2011) \u0093Skills, Tasks and Technologies: Impli-\ncations for Employment and Earnings,\u0094in Ashenfelter, Orley and David Card, eds., Handbook\nof Labor Economics, Volume 4 : Amsterdam, Holland: El Sevier.\nAcemoglu, Daron, David H. Autor, Jonathon Hazell and Pascual Restrepo\n(2021) \u0093AI and Jobs: Evidence from Online Vacancies,\u0094NBER Working Paper No. 28257.\nAcemoglu, Daron and Simon Johnson (2022) Men of Genius: How Hubris Ruined\nTechnology and Prosperity . book manuscript.\nAcemoglu, Daron, Jon Kleinberg and Sendhil Mullainathan (2022) \u0093Problems of\nOnline Communication,\u0094work in progress.\nAcemoglu, Daron, Ali Makhdoumi, Azarakhsh Malekian and Asu Ozdaglar\n(2021) \u0093Too Much Data: Prices and Ine\u00a2 ciencies in Data Markets,\u0094forthcoming American\nEconomic Journal: Micro .\nAcemoglu, Daron, Ali Makhdoumi, Azarakhsh Malekian and Asu Ozdaglar\n(2022) \u0093A Model of Behavioral Manipulation,\u0094work in progress.\nAcemoglu, Daron, Andrea Manera and Pascual Restrepo (2020) \u0093Does the US\nTax Code Favor Automation?\u0094 Brookings Papers on Economic Activity , 2020(1): 231\u0096 285.\nAcemoglu, Daron and Andrew F. Newman (2002) \u0093The Labor Market and Corpo-\nrate Structure,\u0094 European Economic Review 46(10): 1733-1756.\nAcemoglu, Daron, Asu Ozdaglar and James Siderius (2021) \u0093Misinformation:\nStrategic Sharing, Homophily and Endogenous Echo Chambers,\u0094NBER Working Paper No.\n28884.\nAcemoglu, Daron and J\u00f6rn-Ste\u00a4en Pischke (1998) \u0093The Structure of Wages and\n51",
        "82e94b31-58dc-4cb1-93ea-b53955931278": "Investment in Gen. Training,\u0094 Journal of Political Economy, 107(3): 539-572.\nAcemoglu, Daron and James A. Robinson (2006) \u0093Why Did the West Extend the\nFranchise? Democracy, Inequality and Growth in Historical Perspective,\u0094 Quarterly Journal\nof Economics, 115(4): 1167-1199.\nAcemoglu, Daron and James A. Robinson (2006) Economic Origins of Dictatorship\nand Democracy . New York, NY: Cambridge University Press.\nAcemoglu, Daron and James A. Robinson (2008) \u0093Persistence of Power, Elites and\nInstitutions,\u0094 American Economic Review , 98(1): 267-93.\nAcemoglu, Daron and James A. Robinson (2012) Why Nations Fail: The Origins\nof Power, Prosperity, and Poverty. New York, NY: Crown Business.\nAcemoglu, Daron and James A. Robinson (2019) The Narrow Corridor: States,\nSocieties, and the Fate of Liberty. New York, NY: Penguin Press.\nAcemoglu, Daron and Pascual Restrepo (2018) \u0093The Race Between Man and Ma-\nchine: Implications of Technology for Growth, Factor Shares and Employment,\u0094 American\nEconomic Review , 108(6): 1488\u0096 1542.\nAcemoglu, Daron and Pascual Restrepo (2019) \u0093Automation and New Tasks: How\nTechnology Changes Labor Demand,\u0094 Journal of Economic Perspectives , 33(2): 3\u0096 30.\nAcemoglu, Daron and Pascual Restrepo (2021) \u0093Tasks, Automation and the Rise in\nUS Wage Inequality,\u0094NBER Working Paper No. 28920.\nAgarwal, Ajay, Joshua S. Gans and Avi Goldfarb (2018) Prediction Machines: The\nSimple Economics of Arti\u0085cial Intelligence. Cambridge, MA: Harvard Business Review.\nAllcott, Hunt and Matthew Gentzkow (2017) \u0093Social Media and Fake News in the\n2016 Election,\u0094 Journal of Economic Perspectives , 31: 211\u0096 36.\nApplebaum, Ann (2020) Twilight of Democracy: The Seductive Lure of Authoritarian-\nism. New York, NY: Signal.\nAthey, Susan, C. Catalini, and Catherine Tucker (2017) \u0093The Digital Privac Digital:\nSmall Money, Small Costs, Small Talk,\u0094NBER Working Paper No. 23488.\nAutor, David H. (2014) \u0093Skills, Education and the Rise of Earnings Inequality among\nthe Other 99 Percent,\u0094 Science 344(6186): 843-851.\nAutor, David H., Frank Levy and Richard J. Murnane (2003) \u0093The Skill Content\nof Recent Technological Change: An Empirical Exploration,\u0094 Quarterly Journal of Economics,\n118(4): 1279\u0096 1333.\nAutor, David H., David Dorn, and Gordon H. Hanson (2013) \u0093The China Syn-\n52",
        "bc5cea2f-f6f6-4024-95da-908d000967b7": "drome: Local Labor Market E\u00a4ects of Import Competition in the United States,\u0094 American\nEconomic Review 103(6): 2121\u0096 68\nBergemann, Dirk, Alessandro Bonatti, and Tan Gan (2021) \u0093Markets for Infor-\nmation,\u0094Yale mimeo.\nBostrom, Nick (2017) Superintelligence . New York, NY: Danod.\nBrynjolfsson, Erik and Andrew McAfee (2014) The Second Machine Age: Work,\nProgress, and Prosperity in a Time of Brilliant Technologies . New York, NY: W. W. Norton\n& Company.\nBudd, Christopher, Christopher Harris, and John Vickers (1993) \u0093A Model of the\nEvolution of Duopoly: Does the Asymmetry between Firms Tend to Increase or Decrease?\u0094\nReview of Economic Studies 60(3): 543-573.\nChoi, J.P., D.-S. Jeon, and B.-C. Kim (2019) \u0093Privacy and Personal Data Collection\nwith Information Externalities,\u0094 Journal of Public Economics , 173: 113\u0096 124.\nChristian, Brian (2019) The Alignment Problem: Machine Learning and Human Values .\nNew York, NY: WW Norton & Company.\nDreyfus, Hubert L. (1992) What Computers Still Can\u0092 t Do: A Critique of Arti\u0085cial\nReason. Cambridge, MA: MIT Press.\nFarboodi, Maryam, R. Mihet, Thomas Philippon, and Laura Veldkamp (2019)\n\u0093Big Data and Firm Dynamics,\u0094 American Economic Review: Papers and Proceedings ,109,\n38\u0096 42.\nFord, Martin (2015) Rise of Robots . New York, NY: Basic Books.\nForester, Tom (1985) The Information Technology Revolution . Cambridge, MA: MIT\nPress.\nGuriev, Sergei, Emeric Henry, and Ekaterina Zhuravskaya (2020) \u0093Checking and\nSharing Alt-Facts.\u0094mimeo.\nHanson Jon D. and Douglas A. Kysar (1999) \u0093Taking Behavioralism Seriously: Some\nEvidence of Market Manipulation,\u0094 New York University Law Review , 74: 630.\nHuntington, Samuel P. (1991) The Third Wave: Democratization in the Late Twentieth\nCentury . Norman, OK: University of Oklahoma Press.\nJones, Charles I. and Christopher Tonetti (2020) \u0093Non-rivalry in the Economics of\nData,\u0094 American Economic Review, 110(9): 2819-58.\nJudis, John B. (2016) The Populist Explosion: How the Great Recession Transformed\nAmerican and European Politics . New York, NY: Columbia Global Reports.\n53",
        "2b0c1703-80e3-46d8-8588-0588845ab6d3": "Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., & Mullainathan, S.\n(2018) \u0093Human Decisions and Machine Predictions,\u0094 Quarterly Journal of Economics , 133(1):\n237-293.\nKlemperer, Paul (1995) \u0093Competition When Consumers Have Switching Costs: An\nOverview with Applications to Industrial Organization, Macroeconomics and International\nTrade,\u0094 Review of Economic Studies , 62(4): 515-539.\nLanier, Jaron (2018) Ten Arguments for Deleting Your Social Media Accounts Right\nNow. New York, NY: Ho\u00a4mann and Co.\nLee, David S. (2009) \u0093Wage Inequality in the United States During the 1980s: Rising\nThis Version or Falling Minimum Wage?\u0094 Quarterly Journal of Economics, 114(3): 977-1023.\nLee, Kai-Fu (2018) AI superpowers: China, Silicon Valley, and the New World Order .\nNew York, NY: Houghton Mi\u00ad in Harcourt.\nLevy, Roee (2021) \u0093Social Media, News Consumption, and Polarization: Evidence from\na Field Experiment.\u0094 American Economic Review.\nMacCarthy, M .(2016) \u0093New Directions and Privacy: Disclosure, Unfairness and Exter-\nnalities\u0094mimeo. https://ssrn.com/abstract=3093301.\nMarantz, Andrew (2020) Antisocial: Online Extremists, Techno-utopians and the Hi-\njacking of the American Conversation. Penguin, New York, NY\nMishra, P. (2017) Age of Anger: A History of the Present . Macmillan, New York, NY.\nMarko\u00a4, John (1996) Waves of Democracy: Social Movements and Political Change .\nThousands Oaks: Pine Forge Press.\nMosleh, Mohsen, Cameron Martel, Dean Eckles, and David G. Rand (2021)\n\u0093Shared Partisanship Dramatically Increases the Social Tie Formation in a Twitter Field Ex-\nperiment,\u0094 Proceedings of the National Academy of Sciences , 118(7): 1-3.\nNeapolitan, Richard E. and Xia Jiang (2018) Arti\u0085cial Intelligence: With an Intro-\nduction to Machine Learning, 2nd ed. London, UK: Chapman and Hall/CRC.\nNilsson, Nils J. (2009) The Quest for Arti\u0085cial Intelligence: A History of Ideas and\nAchievements. New York, NY: Cambridge University Press.\nPasquale, Frank (2015) The Black Box Society: The Secret Algorithms that Control\nMoney and Information. Cambridge, MA: Harvard University Press.\nPosner, Eric A. (2002) Law and Social Norms . Cambridge, MA: Harvard University\nPress.\nPosner, Eric A. and E. Glen Weyl (2019) Radical Markets. Princeton, NJ: Princeton\n54",
        "a7ed099b-c173-45f0-86b1-1c8b30488ae1": "University Press.\nRussell, Stuart J. and Peter Norvig (2009) Arti\u0085cal Intelligence: A Modern Approach,\n3rd ed. Hoboken, NJ: Prentice Hall.\nRussell, Stuart J. (2019) Human Compatible: Arti\u0085cial Intelligence and the Problem of\nControl . New York, NY: Penguin Press.\nSandel, Michael J. (2020) The Tyranny of Merit: What\u0092 s Become of the Common Good?\nNew York, NY: Penguin Press.\nShapiro, Carl and Joseph E. Stiglitz (1984) \u0093Equilibrium Unemployment As Worker\nDiscipline Device\u0094 American Economic Review 74(3): 433-444.\nSimonite Tom (2020) Algorithms Were Supposed to Fix the Bail System. They Haven\u0092 t.\u0094\nWired. https://www.wired.com/story/algorithms-supposed-\u0085x-bail-system-they-havent/\nSnyder, Timothy (2017) On Tyranny: Twenty Lessons from the Twentieth Century .\nToronton, Ontario: Tim Duggan Books.\nSunstein, Cass (2001) Republic.com , Princeton, NJ: Princeton University Press.\nTirole, Jean (1989) Industrial Organization . Cambridge MA: MIT Press.\nTirole, Jean (1921) \u0093Digital Dystopia,\u0094 American Economic Review , 111(6): 2007-48.\nThompson, Derek (2019) \u0093Should We Be Afraid of AI in the Criminal-Justice System?\u0094\nThe Atlantic . https://www.theatlantic.com/ideas/archive/2019/06/should-we-be-afraid-of-ai-\nin-the-criminal-justice-system/592084/\nVarian, Hal R. (2009) \u0093Economic Aspects of Personal Privacy,\u0094 Internet Policy and\nEconomics , 101\u0096 109. Springer.\nVosoughi, Soroush, Deb Roy, and Sinan Aral (2018) \u0093The Spread of True and False\nNews Online,\u0094 Science , 359: 1146\u0096 1151.\nWest, Darrell M. (2018) The Future of Work: Robots, AI and Automation . , Washing-\nton, DC: Brookings Institution Press.\nZubo\u00a4, Shoshana (2019) The Age of Surveillance Capitalism: The Fight for a Human\nFuture at the New Frontier of Power. London, UK: Pro\u0085le Books.\n55"
    },
    "relevant_docs": {
        "bca54d06-4e9d-4b3d-a2bc-530ad4ae7b49": [
            "0443b8fb-2f8b-4df4-8a21-ff0e570298cf"
        ],
        "9e8a98f7-d3ef-4665-9c79-b3b8f4e83ebf": [
            "0443b8fb-2f8b-4df4-8a21-ff0e570298cf"
        ],
        "bb27adeb-6857-4db6-9bef-2ca25de50b2a": [
            "5cab1b9a-08f6-4d37-9b8d-4db148433719"
        ],
        "6c44ed90-aecd-478f-ab33-707b8312ef14": [
            "5cab1b9a-08f6-4d37-9b8d-4db148433719"
        ],
        "2c6710d4-df01-4488-97fb-a6ccb846c87f": [
            "c6750a30-151b-499b-8592-0c25f25ade03"
        ],
        "42f4e412-eff7-4b38-8fb5-8c84f774a8e7": [
            "c6750a30-151b-499b-8592-0c25f25ade03"
        ],
        "3bbce59e-9498-4e67-b783-8cea245072ac": [
            "98a0ca4d-bc44-4bfe-b67d-b50e3554927c"
        ],
        "e913ab1a-f4cd-4857-ab2e-f236b7ea9750": [
            "98a0ca4d-bc44-4bfe-b67d-b50e3554927c"
        ],
        "687e9c0b-cfca-4a4f-b293-95de7e7cb24a": [
            "0b736938-6721-4587-a648-c0fb52a548b8"
        ],
        "52738215-8d3c-4778-8461-7bccae8081a9": [
            "0b736938-6721-4587-a648-c0fb52a548b8"
        ],
        "795ba67a-09a7-4c49-91a6-625ca2b74765": [
            "8aed6261-7d67-4901-9d2a-d762cf073805"
        ],
        "27898e41-dc57-4668-b44b-54c6693d0b55": [
            "8aed6261-7d67-4901-9d2a-d762cf073805"
        ],
        "b431c5e0-9a9c-435b-a666-de625807ccbe": [
            "19e76f02-e07a-41e4-8990-e6c234f325d6"
        ],
        "1066007f-cd78-47de-967c-545bde93b7e4": [
            "19e76f02-e07a-41e4-8990-e6c234f325d6"
        ],
        "b2485006-74d1-4878-8784-330d779658a0": [
            "686ca3f4-315b-41d7-a4a3-dd342f5715db"
        ],
        "5682702c-04e2-40b5-ae9a-da7de48843f1": [
            "686ca3f4-315b-41d7-a4a3-dd342f5715db"
        ],
        "109d6fa2-806f-4375-95b9-85ec80e3566a": [
            "a7cee37b-c8ad-4be2-9088-064f34e21ac9"
        ],
        "ff4224d3-63a6-4e3f-b878-cdd52af18108": [
            "a7cee37b-c8ad-4be2-9088-064f34e21ac9"
        ],
        "6c344c19-e14f-41f0-84c7-ef09e9a39e70": [
            "c1580fb4-9d05-4d96-ae0e-c77369b8f0c3"
        ],
        "b161d992-e309-4247-b877-af8e9c441924": [
            "c1580fb4-9d05-4d96-ae0e-c77369b8f0c3"
        ],
        "c04602ef-341a-4877-8a6e-12fde69310ad": [
            "168b5393-ee8f-4b57-b067-faeb991ba1e9"
        ],
        "466d4df3-72e5-4885-849c-133dba044cd0": [
            "168b5393-ee8f-4b57-b067-faeb991ba1e9"
        ],
        "5855b89c-622c-439c-8e96-364277b61048": [
            "7ac6f958-31c4-46f3-929e-bdfedfa872bc"
        ],
        "2e5beb9c-36d5-4dcb-9d88-0cf97b9642a7": [
            "7ac6f958-31c4-46f3-929e-bdfedfa872bc"
        ],
        "465a6fb7-07fd-43d4-9b76-c715ff46f88e": [
            "8ea5e54b-3876-4404-80c6-125f69756557"
        ],
        "d4f8a863-c380-4f93-b8ba-6562aea09300": [
            "8ea5e54b-3876-4404-80c6-125f69756557"
        ],
        "f5bc09d2-23f9-41a5-98d1-e7e47aae75b3": [
            "89c50b01-d0e9-4c7c-9fc5-f0350a117756"
        ],
        "f67bf93b-e110-4f58-82c6-f5c27380b4eb": [
            "89c50b01-d0e9-4c7c-9fc5-f0350a117756"
        ],
        "284f1e3b-0e7a-48cd-a015-e74ec5ebf80d": [
            "d2567aaf-b7bd-4a17-82bb-33913fa766ea"
        ],
        "0d578372-2a40-4439-a674-b1f20e755753": [
            "d2567aaf-b7bd-4a17-82bb-33913fa766ea"
        ],
        "da3cdd27-91cc-483b-b93f-1d0354dd5323": [
            "52026d31-b4ed-4b78-9486-5b42c800b785"
        ],
        "75fc2241-1523-41d3-9eec-07bf84f2d95d": [
            "52026d31-b4ed-4b78-9486-5b42c800b785"
        ],
        "bdef3348-5f7a-48e9-acd7-bb0f650cbc83": [
            "b2e20678-89f2-4376-9b2e-a39bdf5d44a7"
        ],
        "0fc4242c-25b4-4dcb-b66d-bb847e1ab2a2": [
            "b2e20678-89f2-4376-9b2e-a39bdf5d44a7"
        ],
        "10924804-2a5c-4c51-953b-1f87e99d0300": [
            "a944efab-88ea-48e9-8942-4fe2d04d8b72"
        ],
        "f18ea29a-e45a-4af1-8ab0-50660629a56e": [
            "a944efab-88ea-48e9-8942-4fe2d04d8b72"
        ],
        "feaf3e75-6a85-498c-b029-98be21e05e04": [
            "be373ab1-454d-47bb-81b7-21e7a9e9b808"
        ],
        "84bea162-4945-483f-9d54-38ee5eae15ee": [
            "be373ab1-454d-47bb-81b7-21e7a9e9b808"
        ],
        "3affde2a-3a0c-4f7f-8db7-f4ef8779e16e": [
            "ce66fc59-06a5-47a5-8a4a-e3392bf234ba"
        ],
        "49a7ec1e-ee6f-4313-8ced-8ad29d2cc78c": [
            "ce66fc59-06a5-47a5-8a4a-e3392bf234ba"
        ],
        "ed8ea40f-1338-46de-be28-c80689e53784": [
            "9f54fb15-b635-474b-9893-16dfb4227a39"
        ],
        "43394af3-d137-4c25-b404-58efb13799ec": [
            "9f54fb15-b635-474b-9893-16dfb4227a39"
        ],
        "7ce63d78-dafe-4d38-bdbc-3dedf6cc3f52": [
            "e1cfa30b-e39c-4366-a030-4178176d239e"
        ],
        "6cfbb017-d20c-4b92-983f-0a6b0b4759e5": [
            "e1cfa30b-e39c-4366-a030-4178176d239e"
        ],
        "026c1172-ff1b-41c2-a2c9-ce0d6604f23a": [
            "68cce1e2-ebd3-4f80-9f3a-2cf7c3e456fa"
        ],
        "a4bdac9c-1095-41ac-a69c-b58ddd0470e9": [
            "68cce1e2-ebd3-4f80-9f3a-2cf7c3e456fa"
        ],
        "8284b8e2-dd9b-49ce-afc3-398b820797c6": [
            "3bb34eb1-cd26-4f62-8e00-9828794b5192"
        ],
        "2bcae304-e4d4-4438-9bf4-d99337239562": [
            "3bb34eb1-cd26-4f62-8e00-9828794b5192"
        ],
        "f4fdbcb5-707d-460c-88c3-4ae71638ad7f": [
            "1db42bb3-284a-49c4-aacf-6e286f84eefa"
        ],
        "3b1a8d8c-ec7f-49a1-bc1d-60facad09c10": [
            "1db42bb3-284a-49c4-aacf-6e286f84eefa"
        ],
        "50dbe296-98f2-4935-8524-01b963404524": [
            "d031f2d3-e8c5-43eb-9c1d-d04f78a1089f"
        ],
        "99e56b80-0750-4ecc-8f49-a4e7a6c2524d": [
            "d031f2d3-e8c5-43eb-9c1d-d04f78a1089f"
        ],
        "f6bb3dfa-e0e4-4aac-b5ac-14b1eba1c33d": [
            "ca58f624-ec0a-49a7-990e-cf9df16fbfce"
        ],
        "c7e16ffa-0f21-4d67-b818-98d5c833cf67": [
            "ca58f624-ec0a-49a7-990e-cf9df16fbfce"
        ],
        "ff85c26d-a6f0-4e62-a0aa-4a5acc589569": [
            "c5a4d653-7d3d-4245-910a-475d0ca2a488"
        ],
        "7a5d1f30-b437-41a8-8051-06870326e757": [
            "c5a4d653-7d3d-4245-910a-475d0ca2a488"
        ],
        "242c36fe-0a5e-4e1c-9e62-3bc1596eb0a1": [
            "4a8ebaf4-91ad-42c8-a06b-fbaf87579606"
        ],
        "cf0c6974-e3ac-491e-9d52-180acc080b7d": [
            "4a8ebaf4-91ad-42c8-a06b-fbaf87579606"
        ],
        "00d54789-c4a4-4d06-ba49-f2e727dc2343": [
            "caec6e46-6891-4f79-8f62-73b52b9010cb"
        ],
        "0502ae94-9171-4203-ac2a-941156479c33": [
            "caec6e46-6891-4f79-8f62-73b52b9010cb"
        ],
        "5b53c728-3ea0-4e20-a696-78130866a02d": [
            "725a1ce3-a8c8-47f3-9938-938e5c462d96"
        ],
        "5559ce21-a69d-49b7-8028-fdaab034430b": [
            "725a1ce3-a8c8-47f3-9938-938e5c462d96"
        ],
        "f0c65f62-7735-426c-93a6-8d857c20faac": [
            "4ed8ded3-fe9b-4b26-a90d-5231fa0cf5bb"
        ],
        "4e73f374-ecab-4f60-9702-15a078db41dd": [
            "4ed8ded3-fe9b-4b26-a90d-5231fa0cf5bb"
        ],
        "4da0c7a7-67b0-4235-9e8d-7d7c1014ca55": [
            "565a37af-9233-4368-8498-e91d6a536157"
        ],
        "77651fee-6d00-4046-a58d-af3c5a64267c": [
            "565a37af-9233-4368-8498-e91d6a536157"
        ],
        "cca67ba4-4175-4428-bb00-ee67667d01d0": [
            "e028c1c8-868a-4881-9dcd-45a7b6299a51"
        ],
        "0081a9a3-e72d-4c0f-b057-0ee08c10aadb": [
            "e028c1c8-868a-4881-9dcd-45a7b6299a51"
        ],
        "7e740d1a-9be8-4c2e-9f11-e377a1d6729a": [
            "afb75264-e191-4f94-9bc4-e8df4d73ff85"
        ],
        "c3c1c4b6-ff7e-4c2d-8608-6c7c121090c4": [
            "afb75264-e191-4f94-9bc4-e8df4d73ff85"
        ],
        "de6af4cb-d006-46f3-af2a-5cb56ef39e49": [
            "04fcb7c8-b9a9-4afc-805d-01d52e92658f"
        ],
        "44cf4e5f-7185-46f0-a1d5-f5638bc42935": [
            "04fcb7c8-b9a9-4afc-805d-01d52e92658f"
        ],
        "39d218a2-1e6a-4c27-bd41-054c78158a4c": [
            "944b16c2-aeb8-4dc6-bf48-7238b396efc8"
        ],
        "97879b67-7368-415e-a6ef-0dd0f06e7a2c": [
            "944b16c2-aeb8-4dc6-bf48-7238b396efc8"
        ],
        "5bcadecf-74c4-426b-852f-8e1dc2d94a53": [
            "69c91987-002b-4bd6-a69c-dd21d9039a00"
        ],
        "d59fadf1-8215-454d-b33b-c42babeb12dd": [
            "69c91987-002b-4bd6-a69c-dd21d9039a00"
        ],
        "0e178987-a3a9-4435-8e9f-f883f27e89d6": [
            "6a3edb5b-2e91-494e-bec4-f3adff629d86"
        ],
        "5314c934-6e45-422a-951c-922eedfde1e8": [
            "6a3edb5b-2e91-494e-bec4-f3adff629d86"
        ],
        "00f6f678-1c66-4c14-a10f-116bfbbef2fa": [
            "8628f9a9-8e6f-4125-ac27-1be0beec0a2a"
        ],
        "ed07a2c8-4c73-427b-9e36-0ee507d6b479": [
            "8628f9a9-8e6f-4125-ac27-1be0beec0a2a"
        ],
        "f5ddcbd7-b010-4216-9eef-c95a4feebc26": [
            "a9ee51cb-3350-4585-a413-d26374f8049d"
        ],
        "f01bc37b-cf38-4d84-8208-351b0d0603e9": [
            "a9ee51cb-3350-4585-a413-d26374f8049d"
        ],
        "0a3caee0-c64b-4f1a-bbf4-2c080db57c05": [
            "23e647eb-bcf9-4126-a6e8-81d645387e2c"
        ],
        "ce1fe31e-b2de-47d2-b6d9-a57c9aa2dc41": [
            "23e647eb-bcf9-4126-a6e8-81d645387e2c"
        ],
        "d38f3cfd-18a5-4721-b40e-118bfcb1b0fe": [
            "57274488-55cf-480c-a0cc-da7c5723dec3"
        ],
        "f1f3fc82-21b7-490e-ae4e-cd14bef29b45": [
            "57274488-55cf-480c-a0cc-da7c5723dec3"
        ],
        "9cbeed63-b268-4d92-b7b7-ceeda8017e93": [
            "fc97c01b-2d8b-46c1-9b93-d125e26d3dd7"
        ],
        "09aeaab0-b32c-4a1f-8047-7eaf0e6babfe": [
            "fc97c01b-2d8b-46c1-9b93-d125e26d3dd7"
        ],
        "c3c5acf2-aa35-4f0d-a72f-aa1c870c009e": [
            "3b8ed764-cc57-4e68-82df-2d9e2b913f4e"
        ],
        "4a7bc943-e44c-4ce7-9de1-12237e1acf77": [
            "3b8ed764-cc57-4e68-82df-2d9e2b913f4e"
        ],
        "2e9decf0-c86a-43a2-9b24-53d9c4adfbcb": [
            "ad13a8aa-9dde-4edf-b76b-ce9f94f52617"
        ],
        "2e854e77-49fc-4a6e-9c81-9234f21ec3ed": [
            "ad13a8aa-9dde-4edf-b76b-ce9f94f52617"
        ],
        "07b79be5-07bb-40b8-b377-161f822c8713": [
            "9187835b-cc3c-4d0b-beb1-fa95431af9cd"
        ],
        "55fc495a-fbff-48a9-91dd-e3f37e43bd8b": [
            "9187835b-cc3c-4d0b-beb1-fa95431af9cd"
        ],
        "12c86618-bbed-48cd-a52f-a91e68745363": [
            "975fd742-824b-496f-a8fc-321f65b47d6e"
        ],
        "dc9e865c-7dd8-4145-aca2-24e716b1c001": [
            "975fd742-824b-496f-a8fc-321f65b47d6e"
        ],
        "29c79b98-049b-4211-83dd-6e2260fa8771": [
            "c0a54865-fd5f-48bf-ba29-3ebaaafb1033"
        ],
        "1e30acbf-2ad6-424d-aee2-1b239d8988df": [
            "c0a54865-fd5f-48bf-ba29-3ebaaafb1033"
        ],
        "d8961c2c-df1d-4b56-9c6a-6a3a91490939": [
            "877ff170-4a87-4437-a6dd-d14e38a6c58f"
        ],
        "d990e4f0-cc31-4ffa-9bec-fe8c2a9735ac": [
            "877ff170-4a87-4437-a6dd-d14e38a6c58f"
        ],
        "7bcc2bfe-5453-4bc8-9460-31bdb85c1279": [
            "323d83bf-566e-43ad-9e1a-7af2d8b97f6f"
        ],
        "8a209704-17ce-489f-9b68-8bf9c77468dd": [
            "323d83bf-566e-43ad-9e1a-7af2d8b97f6f"
        ],
        "aa21c40b-4896-4afe-bdcd-544f263b03bc": [
            "86ba3d86-7356-4094-acd4-7e4e41c9a055"
        ],
        "018cb267-cae5-40c4-bf39-f3429eae3368": [
            "86ba3d86-7356-4094-acd4-7e4e41c9a055"
        ],
        "493e5b47-ddb3-4c51-8779-a9774be9db9a": [
            "4bbe3409-9ff8-49fe-99c1-dc00d3929e83"
        ],
        "44365875-fe39-417c-81c3-46025462cf05": [
            "4bbe3409-9ff8-49fe-99c1-dc00d3929e83"
        ],
        "578299e4-751f-4540-8b7c-68c453c69267": [
            "82e94b31-58dc-4cb1-93ea-b53955931278"
        ],
        "2879b3e1-0d21-40f0-ac87-1220e3224a0c": [
            "82e94b31-58dc-4cb1-93ea-b53955931278"
        ],
        "9e4818e4-cb5b-439e-be6c-94516249e839": [
            "bc5cea2f-f6f6-4024-95da-908d000967b7"
        ],
        "1f791d5d-fafe-4ccb-9531-a2ae15ca853c": [
            "bc5cea2f-f6f6-4024-95da-908d000967b7"
        ],
        "95acb430-91e8-4524-a438-e6de6e267185": [
            "2b0c1703-80e3-46d8-8588-0588845ab6d3"
        ],
        "724e81a3-abf2-41af-a06e-c667495421c7": [
            "2b0c1703-80e3-46d8-8588-0588845ab6d3"
        ],
        "f1d6e1e8-e951-497d-994a-eac8f7aaec82": [
            "a7ed099b-c173-45f0-86b1-1c8b30488ae1"
        ],
        "ddb671ff-22ef-474e-9325-cd2fc63e334d": [
            "a7ed099b-c173-45f0-86b1-1c8b30488ae1"
        ]
    },
    "mode": "text"
}